{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nethome-blazejb/minimol/minimol/model.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  predictor.load_state_dict(torch.load(state_dict_path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "featurizer = Minimol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import pandas as pd\n",
    "from datamol.mol import standardize_smiles\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "from torch.nn import MSELoss as mse_loss\n",
    "from torch.nn import BCEWithLogitsLoss as bce_loss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, samples, task_names):\n",
    "        self.samples = samples['embeddings'].tolist()\n",
    "        self.targets = samples[task_names].fillna(np.nan).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['embeddings'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "cache_path = '.cache/admet_cache.pkl'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(\"Loading from cache...\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "    task_losses = cache_data['task_losses']\n",
    "    test_dataloaders = cache_data['test_dataloaders']\n",
    "    validation_dataloaders = cache_data['validation_dataloaders']\n",
    "    df = cache_data['df']\n",
    "else:\n",
    "    print(\"Cache not found. Running the program...\")\n",
    "    seed = 42\n",
    "    batch_size = 128\n",
    "\n",
    "    group = admet_group(path='admet_data/')\n",
    "    num_mols = 0\n",
    "    task_losses = {}\n",
    "    test_dataloaders = {}\n",
    "    validation_dataloaders = {}\n",
    "    df = pd.DataFrame(columns=['smiles'])\n",
    "\n",
    "    for dataset_i, dataset_name in enumerate(group.dataset_names):\n",
    "        print(f\"{dataset_i + 1} / {len(group.dataset_names)} - {dataset_name}\")\n",
    "        benchmark = group.get(dataset_name)\n",
    "        name = benchmark['name']\n",
    "        mols_test = benchmark['test']\n",
    "\n",
    "        with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull): # Suppress output\n",
    "            mols_train, mols_valid = group.get_train_valid_split(benchmark=name, split_type='default', seed=seed)\n",
    "            mols_test['embeddings'] = featurizer(list(mols_test['Drug']))\n",
    "            mols_valid['embeddings'] = featurizer(list(mols_valid['Drug']))\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            'smiles': mols_train['Drug'],\n",
    "            dataset_name: mols_train['Y']\n",
    "        })\n",
    "\n",
    "        num_mols += len(temp_df)\n",
    "        df = pd.merge(df, temp_df, on='smiles', how='outer')\n",
    "\n",
    "        task_losses[name] = bce_loss() if len(mols_test['Y'].unique()) == 2 else mse_loss()\n",
    "        test_dataloaders[name] = DataLoader(AdmetDataset(mols_test), batch_size=batch_size, shuffle=False)\n",
    "        validation_dataloaders[name] = DataLoader(AdmetDataset(mols_valid), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    df['embeddings'] = featurizer(list(df['smiles']))\n",
    "\n",
    "    cache_data = {\n",
    "        'task_losses': task_losses,\n",
    "        'test_dataloaders': test_dataloaders,\n",
    "        'validation_dataloaders': validation_dataloaders,\n",
    "        'df': df\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(\"Cache saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>caco2_wang</th>\n",
       "      <th>hia_hou</th>\n",
       "      <th>pgp_broccatelli</th>\n",
       "      <th>bioavailability_ma</th>\n",
       "      <th>lipophilicity_astrazeneca</th>\n",
       "      <th>solubility_aqsoldb</th>\n",
       "      <th>bbb_martins</th>\n",
       "      <th>ppbr_az</th>\n",
       "      <th>vdss_lombardo</th>\n",
       "      <th>...</th>\n",
       "      <th>cyp3a4_substrate_carbonmangels</th>\n",
       "      <th>cyp2c9_substrate_carbonmangels</th>\n",
       "      <th>half_life_obach</th>\n",
       "      <th>clearance_microsome_az</th>\n",
       "      <th>clearance_hepatocyte_az</th>\n",
       "      <th>herg</th>\n",
       "      <th>ames</th>\n",
       "      <th>dili</th>\n",
       "      <th>ld50_zhu</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNC1(c2ccccc2Cl)CCCCC1=O</td>\n",
       "      <td>-4.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.1916), tensor(0.3334), tensor(0.8102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNC1(c2ccccc2Cl)CCCCC1=O</td>\n",
       "      <td>-4.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.1916), tensor(0.3334), tensor(0.8102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C/C=C/C/C=C/CCC(=O)[C@@H]1O[C@@H]1C(N)=O</td>\n",
       "      <td>-5.422406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.2584), tensor(0.7886), tensor(1.3459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(NC1(C(=O)N[C@H](Cc2ccccc2)C(=O)NCCCC(=O)N2...</td>\n",
       "      <td>-5.769776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.8761), tensor(0.1187), tensor(1.0743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccccc1)NC...</td>\n",
       "      <td>-7.431799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(2.0538), tensor(1.9126), tensor(1.1178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  caco2_wang  hia_hou  \\\n",
       "0                           CNC1(c2ccccc2Cl)CCCCC1=O   -4.260000      NaN   \n",
       "1                           CNC1(c2ccccc2Cl)CCCCC1=O   -4.260000      NaN   \n",
       "2           C/C=C/C/C=C/CCC(=O)[C@@H]1O[C@@H]1C(N)=O   -5.422406      NaN   \n",
       "3  O=C(NC1(C(=O)N[C@H](Cc2ccccc2)C(=O)NCCCC(=O)N2...   -5.769776      NaN   \n",
       "4  NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccccc1)NC...   -7.431799      NaN   \n",
       "\n",
       "   pgp_broccatelli  bioavailability_ma  lipophilicity_astrazeneca  \\\n",
       "0              NaN                 0.0                        NaN   \n",
       "1              NaN                 0.0                        NaN   \n",
       "2              NaN                 NaN                        NaN   \n",
       "3              NaN                 NaN                        NaN   \n",
       "4              NaN                 NaN                        NaN   \n",
       "\n",
       "   solubility_aqsoldb  bbb_martins  ppbr_az  vdss_lombardo  ...  \\\n",
       "0                 NaN          1.0    44.84            NaN  ...   \n",
       "1                 NaN          1.0    42.01            NaN  ...   \n",
       "2                 NaN          NaN      NaN            NaN  ...   \n",
       "3                 NaN          NaN      NaN            NaN  ...   \n",
       "4                 NaN          NaN      NaN            NaN  ...   \n",
       "\n",
       "   cyp3a4_substrate_carbonmangels  cyp2c9_substrate_carbonmangels  \\\n",
       "0                             NaN                             NaN   \n",
       "1                             NaN                             NaN   \n",
       "2                             NaN                             NaN   \n",
       "3                             NaN                             NaN   \n",
       "4                             NaN                             NaN   \n",
       "\n",
       "   half_life_obach  clearance_microsome_az  clearance_hepatocyte_az  herg  \\\n",
       "0              NaN                     NaN                      NaN   NaN   \n",
       "1              NaN                     NaN                      NaN   NaN   \n",
       "2              NaN                     NaN                      NaN   NaN   \n",
       "3              NaN                     NaN                      NaN   NaN   \n",
       "4              NaN                     NaN                      NaN   NaN   \n",
       "\n",
       "   ames  dili  ld50_zhu                                         embeddings  \n",
       "0   NaN   0.0       NaN  [tensor(1.1916), tensor(0.3334), tensor(0.8102...  \n",
       "1   NaN   0.0       NaN  [tensor(1.1916), tensor(0.3334), tensor(0.8102...  \n",
       "2   NaN   NaN       NaN  [tensor(1.2584), tensor(0.7886), tensor(1.3459...  \n",
       "3   NaN   NaN       NaN  [tensor(1.8761), tensor(0.1187), tensor(1.0743...  \n",
       "4   NaN   NaN       NaN  [tensor(2.0538), tensor(1.9126), tensor(1.1178...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top1_tdc</th>\n",
       "      <th>mole</th>\n",
       "      <th>minimol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>caco2_wang</th>\n",
       "      <td>0.276</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bioavailability_ma</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lipophilicity_astrazeneca</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solubility_aqsoldb</th>\n",
       "      <td>0.761</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hia_hou</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgp_broccatelli</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbb_martins</th>\n",
       "      <td>0.916</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppbr_az</th>\n",
       "      <td>7.526</td>\n",
       "      <td>8.073</td>\n",
       "      <td>7.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdss_lombardo</th>\n",
       "      <td>0.713</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2c9_veith</th>\n",
       "      <td>0.859</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2d6_veith</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp3a4_veith</th>\n",
       "      <td>0.916</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2c9_substrate_carbonmangels</th>\n",
       "      <td>0.474</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2d6_substrate_carbonmangels</th>\n",
       "      <td>0.736</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp3a4_substrate_carbonmangels</th>\n",
       "      <td>0.662</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>half_life_obach</th>\n",
       "      <td>0.562</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clearance_hepatocyte_az</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clearance_microsome_az</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ld50_zhu</th>\n",
       "      <td>0.552</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herg</th>\n",
       "      <td>0.880</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ames</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dili</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                top1_tdc   mole  minimol\n",
       "caco2_wang                         0.276  0.310    0.350\n",
       "bioavailability_ma                 0.748  0.654    0.689\n",
       "lipophilicity_astrazeneca          0.467  0.469    0.456\n",
       "solubility_aqsoldb                 0.761  0.792    0.741\n",
       "hia_hou                            0.989  0.963    0.993\n",
       "pgp_broccatelli                    0.938  0.915    0.942\n",
       "bbb_martins                        0.916  0.903    0.924\n",
       "ppbr_az                            7.526  8.073    7.696\n",
       "vdss_lombardo                      0.713  0.654    0.535\n",
       "cyp2c9_veith                       0.859  0.801    0.823\n",
       "cyp2d6_veith                       0.790  0.682    0.719\n",
       "cyp3a4_veith                       0.916  0.877    0.877\n",
       "cyp2c9_substrate_carbonmangels     0.474  0.446    0.474\n",
       "cyp2d6_substrate_carbonmangels     0.736  0.699    0.695\n",
       "cyp3a4_substrate_carbonmangels     0.662  0.670    0.663\n",
       "half_life_obach                    0.562  0.549    0.495\n",
       "clearance_hepatocyte_az            0.498  0.381    0.446\n",
       "clearance_microsome_az             0.630  0.607    0.628\n",
       "ld50_zhu                           0.552  0.823    0.585\n",
       "herg                               0.880  0.813    0.846\n",
       "ames                               0.871  0.883    0.849\n",
       "dili                               0.925  0.577    0.956"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc_reference = {\n",
    "    \"top1_tdc\": {\n",
    "        \"caco2_wang\": 0.276,\n",
    "        \"bioavailability_ma\": 0.748,\n",
    "        \"lipophilicity_astrazeneca\": 0.467,\n",
    "        \"solubility_aqsoldb\": 0.761,\n",
    "        \"hia_hou\": 0.989,\n",
    "        \"pgp_broccatelli\": 0.938,\n",
    "        \"bbb_martins\": 0.916,\n",
    "        \"ppbr_az\": 7.526,\n",
    "        \"vdss_lombardo\": 0.713,\n",
    "        \"cyp2c9_veith\": 0.859,\n",
    "        \"cyp2d6_veith\": 0.790,\n",
    "        \"cyp3a4_veith\": 0.916,\n",
    "        \"cyp2c9_substrate_carbonmangels\": 0.474,\n",
    "        \"cyp2d6_substrate_carbonmangels\": 0.736,\n",
    "        \"cyp3a4_substrate_carbonmangels\": 0.662,\n",
    "        \"half_life_obach\": 0.562,\n",
    "        \"clearance_hepatocyte_az\": 0.498,\n",
    "        \"clearance_microsome_az\": 0.630,\n",
    "        \"ld50_zhu\": 0.552,\n",
    "        \"herg\": 0.880,\n",
    "        \"ames\": 0.871,\n",
    "        \"dili\": 0.925\n",
    "    },\n",
    "    \"mole\": {\n",
    "        \"caco2_wang\": 0.310,\n",
    "        \"bioavailability_ma\": 0.654,\n",
    "        \"lipophilicity_astrazeneca\": 0.469,\n",
    "        \"solubility_aqsoldb\": 0.792,\n",
    "        \"hia_hou\": 0.963,\n",
    "        \"pgp_broccatelli\": 0.915,\n",
    "        \"bbb_martins\": 0.903,\n",
    "        \"ppbr_az\": 8.073,\n",
    "        \"vdss_lombardo\": 0.654,\n",
    "        \"cyp2c9_veith\": 0.801,\n",
    "        \"cyp2d6_veith\": 0.682,\n",
    "        \"cyp3a4_veith\": 0.877,\n",
    "        \"cyp2c9_substrate_carbonmangels\": 0.446,\n",
    "        \"cyp2d6_substrate_carbonmangels\": 0.699,\n",
    "        \"cyp3a4_substrate_carbonmangels\": 0.670,\n",
    "        \"half_life_obach\": 0.549,\n",
    "        \"clearance_hepatocyte_az\": 0.381,\n",
    "        \"clearance_microsome_az\": 0.607,\n",
    "        \"ld50_zhu\": 0.823,\n",
    "        \"herg\": 0.813,\n",
    "        \"ames\": 0.883,\n",
    "        \"dili\": 0.577\n",
    "    },\n",
    "    \"minimol\": {\n",
    "        \"caco2_wang\": 0.350,\n",
    "        \"bioavailability_ma\": 0.689,\n",
    "        \"lipophilicity_astrazeneca\": 0.456,\n",
    "        \"solubility_aqsoldb\": 0.741,\n",
    "        \"hia_hou\": 0.993,\n",
    "        \"pgp_broccatelli\": 0.942,\n",
    "        \"bbb_martins\": 0.924,\n",
    "        \"ppbr_az\": 7.696,\n",
    "        \"vdss_lombardo\": 0.535,\n",
    "        \"cyp2c9_veith\": 0.823,\n",
    "        \"cyp2d6_veith\": 0.719,\n",
    "        \"cyp3a4_veith\": 0.877,\n",
    "        \"cyp2c9_substrate_carbonmangels\": 0.474,\n",
    "        \"cyp2d6_substrate_carbonmangels\": 0.695,\n",
    "        \"cyp3a4_substrate_carbonmangels\": 0.663,\n",
    "        \"half_life_obach\": 0.495,\n",
    "        \"clearance_hepatocyte_az\": 0.446,\n",
    "        \"clearance_microsome_az\": 0.628,\n",
    "        \"ld50_zhu\": 0.585,\n",
    "        \"herg\": 0.846,\n",
    "        \"ames\": 0.849,\n",
    "        \"dili\": 0.956\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    \"caco2_wang\": \"MAE\",\n",
    "    \"bioavailability_ma\": \"AUROC\",\n",
    "    \"lipophilicity_astrazeneca\": \"MAE\",\n",
    "    \"solubility_aqsoldb\": \"MAE\",\n",
    "    \"hia_hou\": \"AUROC\",\n",
    "    \"pgp_broccatelli\": \"AUROC\",\n",
    "    \"bbb_martins\": \"AUROC\",\n",
    "    \"ppbr_az\": \"MAE\",\n",
    "    \"vdss_lombardo\": \"Spearman\",\n",
    "    \"cyp2c9_veith\": \"AUPRC\",\n",
    "    \"cyp2d6_veith\": \"AUPRC\",\n",
    "    \"cyp3a4_veith\": \"AUPRC\",\n",
    "    \"cyp2c9_substrate_carbonmangels\": \"AUPRC\",\n",
    "    \"cyp2d6_substrate_carbonmangels\": \"AUPRC\",\n",
    "    \"cyp3a4_substrate_carbonmangels\": \"AUROC\",\n",
    "    \"half_life_obach\": \"Spearman\",\n",
    "    \"clearance_hepatocyte_az\": \"Spearman\",\n",
    "    \"clearance_microsome_az\": \"Spearman\",\n",
    "    \"ld50_zhu\": \"MAE\",\n",
    "    \"herg\": \"AUROC\",\n",
    "    \"ames\": \"AUROC\",\n",
    "    \"dili\": \"AUROC\"\n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "result_reference_df = pd.DataFrame(tdc_reference)\n",
    "\n",
    "def evaluate_new_model(df, new_model_dict, new_model_name):\n",
    "    df[new_model_name] = df.index.map(lambda x: new_model_dict[x][0])\n",
    "    df['metrics'] = metrics\n",
    "    def rank_row(row):\n",
    "        metric = metrics.get(row.name)\n",
    "        row_without_rank = row.drop(labels=[f\"{new_model_name}_rank\", \"metrics\"], errors='ignore') \n",
    "        if metric in [\"AUROC\", \"AUPRC\", \"Spearman\"]:\n",
    "            return row_without_rank.rank(ascending=False)\n",
    "        elif metric in [\"MAE\"]:\n",
    "            return row_without_rank.rank(ascending=True)\n",
    "\n",
    "    rank_column = f\"{new_model_name}_rank\"\n",
    "    df[rank_column] = df.apply(rank_row, axis=1)[new_model_name]\n",
    "    \n",
    "    return df\n",
    "\n",
    "result_reference_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "group = admet_group(path='admet_data/')\n",
    "\n",
    "\n",
    "class MultitaskTrainer:\n",
    "    def __init__(self, device, batch_size: int = 256):\n",
    "        self.device = device\n",
    "        self.bs = batch_size\n",
    "        self.task_losses = {}\n",
    "        self.test_dataloaders = {}\n",
    "        self.validation_dataloaders = {}\n",
    "        self.train_dataloader = None\n",
    "    \n",
    "    def set_multitask_dataloader(self, data):\n",
    "        task_names = list(self.task_losses.keys())\n",
    "        self.train_dataloader = DataLoader(MultiDataset(data, task_names), batch_size=124, shuffle=True)\n",
    "\n",
    "    def set_per_task_dataloaders(self, test_loaders, val_loaders, task_losses):\n",
    "        self.task_losses = task_losses\n",
    "        self.test_dataloaders = test_loaders\n",
    "        self.validation_dataloaders = val_loaders\n",
    "\n",
    "    def _eval(self, model, dataloaders, tdc_eval: bool = False):\n",
    "        predictions = {}\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        per_task_losses = torch.tensor([0.0]*len(dataloaders.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "        for task_i, (task_name, dataloader) in enumerate(dataloaders.items()):\n",
    "            predictions[task_name] = []\n",
    "            \n",
    "            for batch_idx, (samples, targets) in enumerate(dataloader):\n",
    "                samples = samples.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                output = model(samples, task=task_name)\n",
    "\n",
    "                loss_fn = self.task_losses[task_name]\n",
    "                task_loss = loss_fn(output.float(), targets.float())\n",
    "                \n",
    "                if tdc_eval:\n",
    "                    if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "                        output = torch.nn.functional.sigmoid(output)\n",
    "                    predictions[task_name] += list(output.detach().cpu())\n",
    "\n",
    "                per_task_losses[task_i] = per_task_losses[task_i] + (task_loss / len(dataloader))\n",
    "                total_loss = total_loss + (task_loss / len(dataloader))\n",
    "\n",
    "        total_loss = total_loss / len(dataloaders.keys())\n",
    "\n",
    "        if tdc_eval:\n",
    "            return total_loss, per_task_losses, predictions \n",
    "        \n",
    "        return total_loss, per_task_losses\n",
    "    \n",
    "    def eval_on_val(self, model):\n",
    "        return self._eval(model, self.validation_dataloaders)\n",
    "\n",
    "    def eval_on_test(self, model):\n",
    "        return self._eval(model, self.test_dataloaders)\n",
    "\n",
    "    def _compute_train_loss(self, outputs, filtered_targets):\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        per_task_losses = torch.tensor([0.0]*len(self.task_losses.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "        for task_i, (task, loss) in enumerate(self.task_losses.items()):\n",
    "            if task not in outputs.keys():\n",
    "                continue\n",
    "            \n",
    "            output = outputs[task]\n",
    "            target = filtered_targets[task]\n",
    "            assert not torch.isnan(output).any(), \"NaNs IN THE OUTPUT!!!\"\n",
    "\n",
    "            task_loss = loss(output.float(), target.float())\n",
    "            total_loss = total_loss + task_loss\n",
    "            per_task_losses[task_i] = task_loss\n",
    "\n",
    "        return total_loss / len(outputs.keys()), per_task_losses\n",
    "\n",
    "    def train(self, model, optimizer, num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_val_loss, per_task_val_loss = self._eval(model, self.validation_dataloaders)\n",
    "        print(f\"Epoch [0 / {num_epochs}], Val loss: {total_val_loss:.2f}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            running_per_task_loss = torch.tensor([0.0]*len(self.task_losses.keys()), requires_grad=False, device=self.device)\n",
    "            for batch_idx, (samples, targets) in enumerate(self.train_dataloader):\n",
    "                samples = samples.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                outputs, filtered_targets = model(samples, targets)\n",
    "                total_loss, per_task_losses = self._compute_train_loss(outputs, filtered_targets)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += total_loss.item()\n",
    "                running_per_task_loss += per_task_losses\n",
    "            \n",
    "            total_val_loss, per_task_val_loss = self._eval(model, self.validation_dataloaders)\n",
    "            print(f\"Epoch [{epoch + 1} / {num_epochs}], Train loss: {running_loss / len(self.train_dataloader):.2f}, Val loss: {total_val_loss:.2f}\")\n",
    "\n",
    "        total_test_loss, per_task_test_loss, predictions = self._eval(model, self.test_dataloaders, tdc_eval=True)\n",
    "        print(f\"Epoch [{epoch + 1} / {num_epochs}], Test loss: {total_test_loss:.2f}\")\n",
    "        tdc_evaluation = group.evaluate_many([predictions]*5)\n",
    "        print(evaluate_new_model(result_reference_df, tdc_evaluation, 'this run'))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trainer = MultitaskTrainer(device)\n",
    "trainer.set_per_task_dataloaders(test_dataloaders, validation_dataloaders, task_losses)\n",
    "trainer.set_multitask_dataloader(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size, head_input_size, hidden_size, depth, dropout):\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        for _ in range(depth - 1):\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, head_input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x += identity\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self, hidden_size, depth, dropout):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.LayerNorm(hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_size, trunk_hidden_size, trunk_depth, head_hidden_size, head_depth, dropout, tasks, uncertainty_weighing: bool = False):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.trunk = ResidualMLP(input_size, head_hidden_size, trunk_hidden_size, trunk_depth, dropout)\n",
    "        self.heads = nn.ModuleDict({task: TaskHead(head_hidden_size, head_depth, dropout) for task in tasks})\n",
    "        self.log_variance = nn.Parameter(torch.tensor(2.0), requires_grad=True) if uncertainty_weighing else None\n",
    "        self.tasks = list(tasks)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, targets=None, task=None):\n",
    "        x = self.trunk(x)\n",
    "        outputs = {}\n",
    "        filtered_targets = {}\n",
    "\n",
    "        if task:\n",
    "            return self.heads[task](x).squeeze()\n",
    "\n",
    "        task_mask = ~torch.isnan(targets)\n",
    "        for idx, task in enumerate(self.tasks): \n",
    "            if task_mask[:, idx].any():\n",
    "                indices = torch.nonzero(task_mask[:, idx], as_tuple=False).squeeze()\n",
    "                outputs[task] = self.heads[task](x[indices]).squeeze()\n",
    "                filtered_targets[task] = targets[indices, idx].squeeze()\n",
    "        return outputs, filtered_targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 10], Val loss: 1557.76\n",
      "Epoch [1 / 10], Train loss: 649.34, Val loss: 1101.61\n",
      "Epoch [2 / 10], Train loss: 667.91, Val loss: 1058.86\n",
      "Epoch [3 / 10], Train loss: 613.77, Val loss: 1064.19\n",
      "Epoch [4 / 10], Train loss: 536.10, Val loss: 1057.59\n",
      "Epoch [5 / 10], Train loss: 426.99, Val loss: 1061.56\n",
      "Epoch [6 / 10], Train loss: 447.92, Val loss: 1066.94\n",
      "Epoch [7 / 10], Train loss: 564.03, Val loss: 1038.85\n",
      "Epoch [8 / 10], Train loss: 473.82, Val loss: 1059.58\n",
      "Epoch [9 / 10], Train loss: 413.23, Val loss: 1056.36\n",
      "Epoch [10 / 10], Train loss: 489.38, Val loss: 1037.92\n",
      "Epoch [10 / 10], Test loss: 241.90\n",
      "                                top1_tdc   mole  minimol  this run   metrics  this run_rank\n",
      "caco2_wang                         0.276  0.310    0.350     0.781       MAE            4.0\n",
      "bioavailability_ma                 0.748  0.654    0.689     0.556     AUROC            4.0\n",
      "lipophilicity_astrazeneca          0.467  0.469    0.456     1.029       MAE            4.0\n",
      "solubility_aqsoldb                 0.761  0.792    0.741     1.617       MAE            4.0\n",
      "hia_hou                            0.989  0.963    0.993     0.404     AUROC            4.0\n",
      "pgp_broccatelli                    0.938  0.915    0.942     0.612     AUROC            4.0\n",
      "bbb_martins                        0.916  0.903    0.924     0.535     AUROC            4.0\n",
      "ppbr_az                            7.526  8.073    7.696    14.637       MAE            4.0\n",
      "vdss_lombardo                      0.713  0.654    0.535     0.071  Spearman            4.0\n",
      "cyp2c9_veith                       0.859  0.801    0.823     0.339     AUPRC            4.0\n",
      "cyp2d6_veith                       0.790  0.682    0.719     0.182     AUPRC            4.0\n",
      "cyp3a4_veith                       0.916  0.877    0.877     0.482     AUPRC            4.0\n",
      "cyp2c9_substrate_carbonmangels     0.474  0.446    0.474     0.272     AUPRC            4.0\n",
      "cyp2d6_substrate_carbonmangels     0.736  0.699    0.695     0.314     AUPRC            4.0\n",
      "cyp3a4_substrate_carbonmangels     0.662  0.670    0.663     0.558     AUROC            4.0\n",
      "half_life_obach                    0.562  0.549    0.495     0.126  Spearman            4.0\n",
      "clearance_hepatocyte_az            0.498  0.381    0.446     0.168  Spearman            4.0\n",
      "clearance_microsome_az             0.630  0.607    0.628    -0.079  Spearman            4.0\n",
      "ld50_zhu                           0.552  0.823    0.585     0.901       MAE            4.0\n",
      "herg                               0.880  0.813    0.846     0.585     AUROC            4.0\n",
      "ames                               0.871  0.883    0.849     0.525     AUROC            4.0\n",
      "dili                               0.925  0.577    0.956     0.481     AUROC            4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss as bce_loss\n",
    "from torch.nn import MSELoss as mse_loss\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "hparams = {\n",
    "    \"trunk_hidden_size\": 512,\n",
    "    \"head_hidden_size\": 128, \n",
    "    \"tasks\": trainer.task_losses.keys(), \n",
    "    \"input_size\": 512, \n",
    "    \"trunk_depth\": 2, \n",
    "    \"head_depth\": 3, \n",
    "    \"dropout\": 0.5\n",
    "}\n",
    "model = MultiTaskModel(**hparams).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9, weight_decay=1e-4)\n",
    "trainer.train(model, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "entity, project = \"blazejba-gc\", \"multitask_pretraining\"\n",
    "run = wandb.init(entity=entity, project=project)\n",
    "\n",
    "# Simulate logging metrics over epochs\n",
    "for epoch in range(10):\n",
    "    accuracy = 0.9 + epoch * 0.01\n",
    "    wandb.log({\"epoch\": epoch, \"accuracy\": accuracy})\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".minimol_p3-12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
