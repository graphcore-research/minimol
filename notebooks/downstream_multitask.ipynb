{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nethome-blazejb/minimol/minimol/model.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  predictor.load_state_dict(torch.load(state_dict_path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "featurizer = Minimol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import pandas as pd\n",
    "from datamol.mol import standardize_smiles\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "from torch.nn import MSELoss as mse_loss\n",
    "from torch.nn import BCEWithLogitsLoss as bce_loss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, samples, task_names):\n",
    "        self.samples = samples['embeddings'].tolist()\n",
    "        self.targets = samples[task_names].fillna(np.nan).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['embeddings'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "cache_path = '.cache/admet_cache.pkl'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(\"Loading from cache...\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "    task_losses = cache_data['task_losses']\n",
    "    test_dataloaders = cache_data['test_dataloaders']\n",
    "    validation_dataloaders = cache_data['validation_dataloaders']\n",
    "    df = cache_data['df']\n",
    "else:\n",
    "    print(\"Cache not found. Running the program...\")\n",
    "    seed = 42\n",
    "    batch_size = 128\n",
    "\n",
    "    group = admet_group(path='admet_data/')\n",
    "    num_mols = 0\n",
    "    task_losses = {}\n",
    "    test_dataloaders = {}\n",
    "    validation_dataloaders = {}\n",
    "    df = pd.DataFrame(columns=['smiles'])\n",
    "\n",
    "    for dataset_i, dataset_name in enumerate(group.dataset_names):\n",
    "        print(f\"{dataset_i + 1} / {len(group.dataset_names)} - {dataset_name}\")\n",
    "        benchmark = group.get(dataset_name)\n",
    "        name = benchmark['name']\n",
    "        mols_test = benchmark['test']\n",
    "\n",
    "        with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull): # Suppress output\n",
    "            mols_train, mols_valid = group.get_train_valid_split(benchmark=name, split_type='default', seed=seed)\n",
    "            mols_test['embeddings'] = featurizer(list(mols_test['Drug']))\n",
    "            mols_valid['embeddings'] = featurizer(list(mols_valid['Drug']))\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            'smiles': mols_train['Drug'],\n",
    "            dataset_name: mols_train['Y']\n",
    "        })\n",
    "\n",
    "        num_mols += len(temp_df)\n",
    "        df = pd.merge(df, temp_df, on='smiles', how='outer')\n",
    "\n",
    "        task_losses[name] = bce_loss() if len(mols_test['Y'].unique()) == 2 else mse_loss()\n",
    "        test_dataloaders[name] = DataLoader(AdmetDataset(mols_test), batch_size=batch_size, shuffle=False)\n",
    "        validation_dataloaders[name] = DataLoader(AdmetDataset(mols_valid), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    df['embeddings'] = featurizer(list(df['smiles']))\n",
    "\n",
    "    cache_data = {\n",
    "        'task_losses': task_losses,\n",
    "        'test_dataloaders': test_dataloaders,\n",
    "        'validation_dataloaders': validation_dataloaders,\n",
    "        'df': df\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(\"Cache saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>caco2_wang</th>\n",
       "      <th>hia_hou</th>\n",
       "      <th>pgp_broccatelli</th>\n",
       "      <th>bioavailability_ma</th>\n",
       "      <th>lipophilicity_astrazeneca</th>\n",
       "      <th>solubility_aqsoldb</th>\n",
       "      <th>bbb_martins</th>\n",
       "      <th>ppbr_az</th>\n",
       "      <th>vdss_lombardo</th>\n",
       "      <th>...</th>\n",
       "      <th>cyp3a4_substrate_carbonmangels</th>\n",
       "      <th>cyp2c9_substrate_carbonmangels</th>\n",
       "      <th>half_life_obach</th>\n",
       "      <th>clearance_microsome_az</th>\n",
       "      <th>clearance_hepatocyte_az</th>\n",
       "      <th>herg</th>\n",
       "      <th>ames</th>\n",
       "      <th>dili</th>\n",
       "      <th>ld50_zhu</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNC1(c2ccccc2Cl)CCCCC1=O</td>\n",
       "      <td>-4.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.1916), tensor(0.3334), tensor(0.8102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNC1(c2ccccc2Cl)CCCCC1=O</td>\n",
       "      <td>-4.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.1916), tensor(0.3334), tensor(0.8102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C/C=C/C/C=C/CCC(=O)[C@@H]1O[C@@H]1C(N)=O</td>\n",
       "      <td>-5.422406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.2584), tensor(0.7886), tensor(1.3459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(NC1(C(=O)N[C@H](Cc2ccccc2)C(=O)NCCCC(=O)N2...</td>\n",
       "      <td>-5.769776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.8761), tensor(0.1187), tensor(1.0743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccccc1)NC...</td>\n",
       "      <td>-7.431799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(2.0538), tensor(1.9126), tensor(1.1178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  caco2_wang  hia_hou  \\\n",
       "0                           CNC1(c2ccccc2Cl)CCCCC1=O   -4.260000      NaN   \n",
       "1                           CNC1(c2ccccc2Cl)CCCCC1=O   -4.260000      NaN   \n",
       "2           C/C=C/C/C=C/CCC(=O)[C@@H]1O[C@@H]1C(N)=O   -5.422406      NaN   \n",
       "3  O=C(NC1(C(=O)N[C@H](Cc2ccccc2)C(=O)NCCCC(=O)N2...   -5.769776      NaN   \n",
       "4  NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccccc1)NC...   -7.431799      NaN   \n",
       "\n",
       "   pgp_broccatelli  bioavailability_ma  lipophilicity_astrazeneca  \\\n",
       "0              NaN                 0.0                        NaN   \n",
       "1              NaN                 0.0                        NaN   \n",
       "2              NaN                 NaN                        NaN   \n",
       "3              NaN                 NaN                        NaN   \n",
       "4              NaN                 NaN                        NaN   \n",
       "\n",
       "   solubility_aqsoldb  bbb_martins  ppbr_az  vdss_lombardo  ...  \\\n",
       "0                 NaN          1.0    44.84            NaN  ...   \n",
       "1                 NaN          1.0    42.01            NaN  ...   \n",
       "2                 NaN          NaN      NaN            NaN  ...   \n",
       "3                 NaN          NaN      NaN            NaN  ...   \n",
       "4                 NaN          NaN      NaN            NaN  ...   \n",
       "\n",
       "   cyp3a4_substrate_carbonmangels  cyp2c9_substrate_carbonmangels  \\\n",
       "0                             NaN                             NaN   \n",
       "1                             NaN                             NaN   \n",
       "2                             NaN                             NaN   \n",
       "3                             NaN                             NaN   \n",
       "4                             NaN                             NaN   \n",
       "\n",
       "   half_life_obach  clearance_microsome_az  clearance_hepatocyte_az  herg  \\\n",
       "0              NaN                     NaN                      NaN   NaN   \n",
       "1              NaN                     NaN                      NaN   NaN   \n",
       "2              NaN                     NaN                      NaN   NaN   \n",
       "3              NaN                     NaN                      NaN   NaN   \n",
       "4              NaN                     NaN                      NaN   NaN   \n",
       "\n",
       "   ames  dili  ld50_zhu                                         embeddings  \n",
       "0   NaN   0.0       NaN  [tensor(1.1916), tensor(0.3334), tensor(0.8102...  \n",
       "1   NaN   0.0       NaN  [tensor(1.1916), tensor(0.3334), tensor(0.8102...  \n",
       "2   NaN   NaN       NaN  [tensor(1.2584), tensor(0.7886), tensor(1.3459...  \n",
       "3   NaN   NaN       NaN  [tensor(1.8761), tensor(0.1187), tensor(1.0743...  \n",
       "4   NaN   NaN       NaN  [tensor(2.0538), tensor(1.9126), tensor(1.1178...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdc1</th>\n",
       "      <th>tdc2</th>\n",
       "      <th>tdc3</th>\n",
       "      <th>tdc4</th>\n",
       "      <th>tdc5</th>\n",
       "      <th>mole</th>\n",
       "      <th>minimol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>caco2_wang</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bioavailability_ma</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lipophilicity_astrazeneca</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solubility_aqsoldb</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hia_hou</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgp_broccatelli</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbb_martins</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppbr_az</th>\n",
       "      <td>7.53</td>\n",
       "      <td>7.66</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.91</td>\n",
       "      <td>8.29</td>\n",
       "      <td>8.07</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vdss_lombardo</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2c9_veith</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2d6_veith</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp3a4_veith</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2c9_substrate_carbonmangels</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp2d6_substrate_carbonmangels</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyp3a4_substrate_carbonmangels</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>half_life_obach</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clearance_hepatocyte_az</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clearance_microsome_az</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ld50_zhu</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herg</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ames</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dili</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tdc1  tdc2  tdc3  tdc4  tdc5  mole  minimol\n",
       "caco2_wang                      0.28  0.28  0.29  0.29  0.29  0.31     0.35\n",
       "bioavailability_ma              0.75  0.74  0.73  0.71  0.67  0.65     0.69\n",
       "lipophilicity_astrazeneca       0.47  0.47  0.48  0.53  0.54  0.47     0.46\n",
       "solubility_aqsoldb              0.76  0.78  0.79  0.79  0.83  0.79     0.74\n",
       "hia_hou                         0.99  0.99  0.99  0.98  0.98  0.96     0.99\n",
       "pgp_broccatelli                 0.94  0.94  0.93  0.93  0.93  0.92     0.94\n",
       "bbb_martins                     0.92  0.92  0.91  0.91  0.91  0.90     0.92\n",
       "ppbr_az                         7.53  7.66  7.79  7.91  8.29  8.07     7.70\n",
       "vdss_lombardo                   0.71  0.71  0.63  0.61  0.58  0.65     0.54\n",
       "cyp2c9_veith                    0.86  0.84  0.83  0.79  0.78  0.80     0.82\n",
       "cyp2d6_veith                    0.79  0.74  0.72  0.72  0.67  0.68     0.72\n",
       "cyp3a4_veith                    0.92  0.90  0.90  0.88  0.88  0.88     0.88\n",
       "cyp2c9_substrate_carbonmangels  0.47  0.44  0.44  0.43  0.41  0.45     0.47\n",
       "cyp2d6_substrate_carbonmangels  0.74  0.72  0.71  0.70  0.69  0.70     0.69\n",
       "cyp3a4_substrate_carbonmangels  0.66  0.65  0.65  0.64  0.64  0.67     0.66\n",
       "half_life_obach                 0.56  0.56  0.55  0.54  0.44  0.55     0.49\n",
       "clearance_hepatocyte_az         0.50  0.47  0.44  0.44  0.43  0.38     0.45\n",
       "clearance_microsome_az          0.63  0.63  0.62  0.60  0.60  0.61     0.63\n",
       "ld50_zhu                        0.55  0.59  0.61  0.62  0.62  0.82     0.58\n",
       "herg                            0.88  0.87  0.87  0.86  0.84  0.81     0.85\n",
       "ames                            0.87  0.87  0.87  0.85  0.84  0.88     0.85\n",
       "dili                            0.93  0.92  0.92  0.91  0.90  0.58     0.96"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc_reference = {\n",
    "    \"tdc1\": {\"caco2_wang\": 0.276, \"bioavailability_ma\": 0.748, \"lipophilicity_astrazeneca\": 0.467, \"solubility_aqsoldb\": 0.761, \"hia_hou\": 0.989, \"pgp_broccatelli\": 0.938, \"bbb_martins\": 0.916, \"ppbr_az\": 7.526, \"vdss_lombardo\": 0.713, \"cyp2c9_veith\": 0.859, \"cyp2d6_veith\": 0.790, \"cyp3a4_veith\": 0.916, \"cyp2c9_substrate_carbonmangels\": 0.474, \"cyp2d6_substrate_carbonmangels\": 0.736, \"cyp3a4_substrate_carbonmangels\": 0.662, \"half_life_obach\": 0.562, \"clearance_hepatocyte_az\": 0.498, \"clearance_microsome_az\": 0.630, \"ld50_zhu\": 0.552, \"herg\": 0.880, \"ames\": 0.871, \"dili\": 0.925},\n",
    "    \"tdc2\": {'caco2_wang': 0.285, 'bioavailability_ma': 0.742, 'lipophilicity_astrazeneca': 0.470, 'solubility_aqsoldb': 0.776, 'hia_hou': 0.988, 'pgp_broccatelli': 0.935, 'bbb_martins': 0.915, 'ppbr_az': 7.660, 'vdss_lombardo': 0.707, 'cyp2c9_veith': 0.839, 'cyp2d6_veith': 0.739, 'cyp3a4_veith': 0.904, 'cyp2c9_substrate_carbonmangels': 0.437, 'cyp2d6_substrate_carbonmangels': 0.720, 'cyp3a4_substrate_carbonmangels': 0.650, 'half_life_obach': 0.557, 'clearance_hepatocyte_az': 0.466, 'clearance_microsome_az': 0.626, 'ld50_zhu': 0.588, 'herg': 0.874, 'ames': 0.869, 'dili': 0.919},\n",
    "    \"tdc3\": {'caco2_wang': 0.287, 'bioavailability_ma': 0.730, 'lipophilicity_astrazeneca': 0.479, 'solubility_aqsoldb': 0.789, 'hia_hou': 0.986, 'pgp_broccatelli': 0.930, 'bbb_martins': 0.913, 'ppbr_az': 7.788, 'vdss_lombardo': 0.627, 'cyp2c9_veith': 0.829, 'cyp2d6_veith': 0.723, 'cyp3a4_veith': 0.902, 'cyp2c9_substrate_carbonmangels': 0.437, 'cyp2d6_substrate_carbonmangels': 0.713, 'cyp3a4_substrate_carbonmangels': 0.647, 'half_life_obach': 0.547, 'clearance_hepatocyte_az': 0.440, 'clearance_microsome_az': 0.625, 'ld50_zhu': 0.606, 'herg': 0.871, 'ames': 0.868, 'dili': 0.917},\n",
    "    \"tdc4\": {'caco2_wang': 0.287, 'bioavailability_ma': 0.706, 'lipophilicity_astrazeneca': 0.525, 'solubility_aqsoldb': 0.792, 'hia_hou': 0.981, 'pgp_broccatelli': 0.929, 'bbb_martins': 0.912, 'ppbr_az': 7.914, 'vdss_lombardo': 0.609, 'cyp2c9_veith': 0.786, 'cyp2d6_veith': 0.721, 'cyp3a4_veith': 0.881, 'cyp2c9_substrate_carbonmangels': 0.433, 'cyp2d6_substrate_carbonmangels': 0.704, 'cyp3a4_substrate_carbonmangels': 0.640, 'half_life_obach': 0.544, 'clearance_hepatocyte_az': 0.439, 'clearance_microsome_az': 0.599, 'ld50_zhu': 0.621, 'herg': 0.856, 'ames': 0.850, 'dili': 0.909},\n",
    "    \"tdc5\": {'caco2_wang': 0.289, 'bioavailability_ma': 0.672, 'lipophilicity_astrazeneca': 0.535, 'solubility_aqsoldb': 0.827, 'hia_hou': 0.978, 'pgp_broccatelli': 0.929, 'bbb_martins': 0.910, 'ppbr_az': 8.288, 'vdss_lombardo': 0.582, 'cyp2c9_veith': 0.783, 'cyp2d6_veith': 0.673, 'cyp3a4_veith': 0.876, 'cyp2c9_substrate_carbonmangels': 0.415, 'cyp2d6_substrate_carbonmangels': 0.686, 'cyp3a4_substrate_carbonmangels': 0.639, 'half_life_obach': 0.438, 'clearance_hepatocyte_az': 0.431, 'clearance_microsome_az': 0.597, 'ld50_zhu': 0.622, 'herg': 0.841, 'ames': 0.842, 'dili': 0.899},\n",
    "    \"mole\":     {\"caco2_wang\": 0.310, \"bioavailability_ma\": 0.654, \"lipophilicity_astrazeneca\": 0.469, \"solubility_aqsoldb\": 0.792, \"hia_hou\": 0.963, \"pgp_broccatelli\": 0.915, \"bbb_martins\": 0.903, \"ppbr_az\": 8.073, \"vdss_lombardo\": 0.654, \"cyp2c9_veith\": 0.801, \"cyp2d6_veith\": 0.682, \"cyp3a4_veith\": 0.877, \"cyp2c9_substrate_carbonmangels\": 0.446, \"cyp2d6_substrate_carbonmangels\": 0.699, \"cyp3a4_substrate_carbonmangels\": 0.670, \"half_life_obach\": 0.549, \"clearance_hepatocyte_az\": 0.381, \"clearance_microsome_az\": 0.607, \"ld50_zhu\": 0.823, \"herg\": 0.813, \"ames\": 0.883, \"dili\": 0.577},\n",
    "    \"minimol\":  {\"caco2_wang\": 0.350, \"bioavailability_ma\": 0.689, \"lipophilicity_astrazeneca\": 0.456, \"solubility_aqsoldb\": 0.741, \"hia_hou\": 0.993, \"pgp_broccatelli\": 0.942, \"bbb_martins\": 0.924, \"ppbr_az\": 7.696, \"vdss_lombardo\": 0.535, \"cyp2c9_veith\": 0.823, \"cyp2d6_veith\": 0.719, \"cyp3a4_veith\": 0.877, \"cyp2c9_substrate_carbonmangels\": 0.474, \"cyp2d6_substrate_carbonmangels\": 0.695, \"cyp3a4_substrate_carbonmangels\": 0.663, \"half_life_obach\": 0.495, \"clearance_hepatocyte_az\": 0.446, \"clearance_microsome_az\": 0.628, \"ld50_zhu\": 0.585, \"herg\": 0.846, \"ames\": 0.849, \"dili\": 0.956}\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    \"caco2_wang\"                    : \"MAE\",\n",
    "    \"bioavailability_ma\"            : \"AUROC\",\n",
    "    \"lipophilicity_astrazeneca\"     : \"MAE\",\n",
    "    \"solubility_aqsoldb\"            : \"MAE\",\n",
    "    \"hia_hou\"                       : \"AUROC\",\n",
    "    \"pgp_broccatelli\"               : \"AUROC\",\n",
    "    \"bbb_martins\"                   : \"AUROC\",\n",
    "    \"ppbr_az\"                       : \"MAE\",\n",
    "    \"vdss_lombardo\"                 : \"Spearman\",\n",
    "    \"cyp2c9_veith\"                  : \"AUPRC\",\n",
    "    \"cyp2d6_veith\"                  : \"AUPRC\",\n",
    "    \"cyp3a4_veith\"                  : \"AUPRC\",\n",
    "    \"cyp2c9_substrate_carbonmangels\": \"AUPRC\",\n",
    "    \"cyp2d6_substrate_carbonmangels\": \"AUPRC\",\n",
    "    \"cyp3a4_substrate_carbonmangels\": \"AUROC\",\n",
    "    \"half_life_obach\"               : \"Spearman\",\n",
    "    \"clearance_hepatocyte_az\"       : \"Spearman\",\n",
    "    \"clearance_microsome_az\"        : \"Spearman\",\n",
    "    \"ld50_zhu\"                      : \"MAE\",\n",
    "    \"herg\"                          : \"AUROC\",\n",
    "    \"ames\"                          : \"AUROC\",\n",
    "    \"dili\"                          : \"AUROC\"\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "result_reference_df = pd.DataFrame(tdc_reference)\n",
    "\n",
    "def evaluate_new_model(df, new_model_dict, new_model_name):\n",
    "    df['metrics'] = metrics\n",
    "    df[new_model_name] = df.index.map(lambda x: new_model_dict[x][0])\n",
    "    def rank_row(row):\n",
    "        metric = metrics.get(row.name)\n",
    "        row_without_rank = row.drop(labels=[f\"{new_model_name}_rank\", \"metrics\"], errors='ignore') \n",
    "        if metric in [\"AUROC\", \"AUPRC\", \"Spearman\"]:\n",
    "            return row_without_rank.rank(ascending=False)\n",
    "        elif metric in [\"MAE\"]:\n",
    "            return row_without_rank.rank(ascending=True)\n",
    "\n",
    "    rank_column = f\"{new_model_name}_rank\"\n",
    "    df[rank_column] = df.apply(rank_row, axis=1)[new_model_name]\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols.insert(0, cols.pop(cols.index('metrics')))\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_scores(df, models: list = ['tdc1', 'tdc2', 'tdc3', 'tdc4', 'tdc5', 'mole', 'minimol', 'this_run']):\n",
    "    global metrics\n",
    "\n",
    "    scores = {}\n",
    "    for model in models:\n",
    "        total_score = 0\n",
    "        for dataset, metric in metrics.items():\n",
    "            score = df.loc[dataset, model]\n",
    "            score_min = df.loc[dataset, models].min()\n",
    "            score_max = df.loc[dataset, models].max()\n",
    "            total_score += score if metric in ['AUROC', 'AUPRC', 'Spearman'] else 1 - (score - score_min) / (score_max - score_min)\n",
    "        scores[model] = total_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "result_reference_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "group = admet_group(path='admet_data/')\n",
    "\n",
    "\n",
    "class MultitaskTrainer:\n",
    "    def __init__(self, device, batch_size: int = 256):\n",
    "        self.device = device\n",
    "        self.bs = batch_size\n",
    "        self.task_losses = {}\n",
    "        self.test_dataloaders = {}\n",
    "        self.validation_dataloaders = {}\n",
    "        self.train_dataloader = None\n",
    "    \n",
    "    def set_multitask_dataloader(self, data):\n",
    "        task_names = list(self.task_losses.keys())\n",
    "        self.train_dataloader = DataLoader(MultiDataset(data, task_names), batch_size=124, shuffle=True)\n",
    "\n",
    "    def set_per_task_dataloaders(self, test_loaders, val_loaders, task_losses):\n",
    "        self.task_losses = task_losses\n",
    "        self.test_dataloaders = test_loaders\n",
    "        self.validation_dataloaders = val_loaders\n",
    "\n",
    "    def _eval(self, model, dataloaders, tdc_eval: bool = False):\n",
    "        model.eval()\n",
    "\n",
    "        predictions = {}\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        per_task_losses = torch.tensor([0.0]*len(dataloaders.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "        for task_i, (task_name, dataloader) in enumerate(dataloaders.items()):\n",
    "            predictions[task_name] = []\n",
    "            \n",
    "            for batch_idx, (samples, targets) in enumerate(dataloader):\n",
    "                samples = samples.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                output = model(samples, task=task_name)\n",
    "\n",
    "                loss_fn = self.task_losses[task_name]\n",
    "                task_loss = loss_fn(output.float(), targets.float())\n",
    "                \n",
    "                if tdc_eval:\n",
    "                    if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "                        output = torch.nn.functional.sigmoid(output)\n",
    "                    predictions[task_name] += list(output.detach().cpu())\n",
    "\n",
    "                per_task_losses[task_i] = per_task_losses[task_i] + (task_loss / len(dataloader))\n",
    "                total_loss = total_loss + (task_loss / len(dataloader))\n",
    "\n",
    "        total_loss = total_loss / len(dataloaders.keys())\n",
    "\n",
    "        if tdc_eval:\n",
    "            return total_loss, per_task_losses, predictions \n",
    "        \n",
    "        return total_loss, per_task_losses\n",
    "    \n",
    "    def eval_on_val(self, model):\n",
    "        return self._eval(model, self.validation_dataloaders)\n",
    "\n",
    "    def eval_on_test(self, model):\n",
    "        return self._eval(model, self.test_dataloaders)\n",
    "\n",
    "    def _compute_train_loss(self, outputs, filtered_targets):\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        per_task_losses = torch.tensor([0.0]*len(self.task_losses.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "        for task_i, (task, loss) in enumerate(self.task_losses.items()):\n",
    "            if task not in outputs.keys():\n",
    "                continue\n",
    "            \n",
    "            output = outputs[task]\n",
    "            target = filtered_targets[task]\n",
    "            assert not torch.isnan(output).any(), \"NaNs IN THE OUTPUT!!!\"\n",
    "\n",
    "            task_loss = loss(output.float(), target.float())\n",
    "            total_loss = total_loss + task_loss\n",
    "            per_task_losses[task_i] = task_loss\n",
    "\n",
    "        return total_loss / len(outputs.keys()), per_task_losses\n",
    "\n",
    "    def train(self, model, optimizer, scheduler, num_epochs, n_freeze_epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_val_loss, per_task_val_loss = self._eval(model, self.validation_dataloaders)\n",
    "        print(f\"Epoch [0 / {num_epochs}], Val loss: {total_val_loss:.2f}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            if epoch > num_epochs - n_freeze_epochs:\n",
    "                model.freeze_trunk()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_per_task_loss = torch.tensor([0.0]*len(self.task_losses.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "            for batch_idx, (samples, targets) in enumerate(self.train_dataloader):\n",
    "                samples = samples.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                outputs, filtered_targets = model(samples, targets)\n",
    "                total_loss, per_task_losses = self._compute_train_loss(outputs, filtered_targets)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += total_loss.item()\n",
    "                running_per_task_loss += per_task_losses\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            total_val_loss, per_task_val_loss = self._eval(model, self.validation_dataloaders)\n",
    "            print(f\"Epoch [{epoch + 1} / {num_epochs}], Train loss: {running_loss / len(self.train_dataloader):.2f}, Val loss: {total_val_loss:.2f}\")\n",
    "\n",
    "        total_test_loss, per_task_test_loss, predictions = self._eval(model, self.test_dataloaders, tdc_eval=True)\n",
    "        print(f\"Epoch [{epoch + 1} / {num_epochs}], Test loss: {total_test_loss:.2f}\")\n",
    "        self._compare_results(predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_results(predictions):\n",
    "        tdc_evaluation = group.evaluate_many([predictions]*5)\n",
    "        results_df = evaluate_new_model(result_reference_df, tdc_evaluation, 'this_run')\n",
    "        scores_row = pd.DataFrame(get_scores(results_df), index=['Scores'])\n",
    "        results_df = pd.concat([results_df, scores_row], axis=0)\n",
    "\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.expand_frame_repr', False)\n",
    "        pd.options.display.float_format = '{:.2f}'.format\n",
    "        print(results_df)\n",
    "\n",
    "def trainer_factory(batch_size: int = 128):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trainer = MultitaskTrainer(device)\n",
    "    trainer.set_per_task_dataloaders(test_dataloaders, validation_dataloaders, task_losses)\n",
    "    trainer.set_multitask_dataloader(df)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth, dropout):\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        for _ in range(depth - 1):\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x += identity\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, depth, dropout):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.layers.append(nn.LayerNorm(hidden_dim))\n",
    "        for _ in range(depth - 1):\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nn.LayerNorm(hidden_dim))\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x += identity\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 head_params         : dict,\n",
    "                 trunk_params        : dict, \n",
    "                 dropout             : float,\n",
    "                 tasks               : Union[tuple, list], \n",
    "                 uncertainty_weighing: bool = False):\n",
    "\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "\n",
    "        if trunk_params['depth'] > 0:\n",
    "            self.trunk = ResidualMLP(dropout=dropout, **trunk_params)\n",
    "        else:\n",
    "            assert trunk_params['input_dim'] == head_params['input_dim'], \"Input size must match when trunk depth is 0.\"\n",
    "            self.trunk = None\n",
    "        \n",
    "        self.heads = nn.ModuleDict({task: TaskHead(dropout=dropout, **head_params) for task in tasks})\n",
    "        self.log_variance = nn.Parameter(torch.tensor(2.0), requires_grad=True) if uncertainty_weighing else None\n",
    "        self.tasks = list(tasks)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def freeze_trunk(self):\n",
    "        if self.trunk:\n",
    "            for param in self.trunk.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, targets=None, task=None):\n",
    "        x = self.trunk(x) if self.trunk else x\n",
    "        \n",
    "        outputs = {}\n",
    "        filtered_targets = {}\n",
    "\n",
    "        if task:\n",
    "            return self.heads[task](x).squeeze()\n",
    "\n",
    "        task_mask = ~torch.isnan(targets)\n",
    "        for idx, task in enumerate(self.tasks): \n",
    "            if task_mask[:, idx].any():\n",
    "                indices = torch.nonzero(task_mask[:, idx], as_tuple=False).squeeze()\n",
    "                outputs[task] = self.heads[task](x[indices]).squeeze()\n",
    "                filtered_targets[task] = targets[indices, idx].squeeze()\n",
    "\n",
    "        return outputs, filtered_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 20], Val loss: 1566.21\n",
      "Epoch [1 / 20], Train loss: 934.92, Val loss: 1539.21\n",
      "Epoch [2 / 20], Train loss: 499.76, Val loss: 1035.63\n",
      "Epoch [3 / 20], Train loss: 462.32, Val loss: 1024.52\n",
      "Epoch [4 / 20], Train loss: 418.21, Val loss: 987.14\n",
      "Epoch [5 / 20], Train loss: 397.27, Val loss: 984.20\n",
      "Epoch [6 / 20], Train loss: 464.40, Val loss: 1043.79\n",
      "Epoch [7 / 20], Train loss: 348.19, Val loss: 1003.67\n",
      "Epoch [8 / 20], Train loss: 461.35, Val loss: 982.85\n",
      "Epoch [9 / 20], Train loss: 375.08, Val loss: 938.08\n",
      "Epoch [10 / 20], Train loss: 334.84, Val loss: 989.84\n",
      "Epoch [11 / 20], Train loss: 915.14, Val loss: 1018.72\n",
      "Epoch [12 / 20], Train loss: 308.75, Val loss: 895.22\n",
      "Epoch [13 / 20], Train loss: 284.77, Val loss: 817.78\n",
      "Epoch [14 / 20], Train loss: 289.26, Val loss: 805.19\n",
      "Epoch [15 / 20], Train loss: 301.03, Val loss: 805.07\n",
      "Epoch [16 / 20], Train loss: 226.54, Val loss: 816.26\n",
      "Epoch [17 / 20], Train loss: 232.52, Val loss: 795.15\n",
      "Epoch [18 / 20], Train loss: 136.09, Val loss: 793.16\n",
      "Epoch [19 / 20], Train loss: 196.55, Val loss: 787.51\n",
      "Epoch [20 / 20], Train loss: 161.06, Val loss: 789.58\n",
      "Epoch [20 / 20], Test loss: 184.96\n",
      "                                 metrics  tdc1  tdc2  tdc3  tdc4  tdc5  mole  minimol  this_run  this_run_rank\n",
      "caco2_wang                           MAE  0.28  0.28  0.29  0.29  0.29  0.31     0.35      0.38           8.00\n",
      "bioavailability_ma                 AUROC  0.75  0.74  0.73  0.71  0.67  0.65     0.69      0.61           8.00\n",
      "lipophilicity_astrazeneca            MAE  0.47  0.47  0.48  0.53  0.54  0.47     0.46      0.56           8.00\n",
      "solubility_aqsoldb                   MAE  0.76  0.78  0.79  0.79  0.83  0.79     0.74      0.96           8.00\n",
      "hia_hou                            AUROC  0.99  0.99  0.99  0.98  0.98  0.96     0.99      0.97           7.00\n",
      "pgp_broccatelli                    AUROC  0.94  0.94  0.93  0.93  0.93  0.92     0.94      0.94           2.00\n",
      "bbb_martins                        AUROC  0.92  0.92  0.91  0.91  0.91  0.90     0.92      0.90           7.00\n",
      "ppbr_az                              MAE  7.53  7.66  7.79  7.91  8.29  8.07     7.70      8.45           8.00\n",
      "vdss_lombardo                   Spearman  0.71  0.71  0.63  0.61  0.58  0.65     0.54      0.38           8.00\n",
      "cyp2c9_veith                       AUPRC  0.86  0.84  0.83  0.79  0.78  0.80     0.82      0.79           6.00\n",
      "cyp2d6_veith                       AUPRC  0.79  0.74  0.72  0.72  0.67  0.68     0.72      0.67           8.00\n",
      "cyp3a4_veith                       AUPRC  0.92  0.90  0.90  0.88  0.88  0.88     0.88      0.85           8.00\n",
      "cyp2c9_substrate_carbonmangels     AUPRC  0.47  0.44  0.44  0.43  0.41  0.45     0.47      0.42           7.00\n",
      "cyp2d6_substrate_carbonmangels     AUPRC  0.74  0.72  0.71  0.70  0.69  0.70     0.69      0.67           8.00\n",
      "cyp3a4_substrate_carbonmangels     AUROC  0.66  0.65  0.65  0.64  0.64  0.67     0.66      0.64           6.00\n",
      "half_life_obach                 Spearman  0.56  0.56  0.55  0.54  0.44  0.55     0.49      0.39           8.00\n",
      "clearance_hepatocyte_az         Spearman  0.50  0.47  0.44  0.44  0.43  0.38     0.45      0.49           2.00\n",
      "clearance_microsome_az          Spearman  0.63  0.63  0.62  0.60  0.60  0.61     0.63      0.60           6.00\n",
      "ld50_zhu                             MAE  0.55  0.59  0.61  0.62  0.62  0.82     0.58      0.83           8.00\n",
      "herg                               AUROC  0.88  0.87  0.87  0.86  0.84  0.81     0.85      0.83           7.00\n",
      "ames                               AUROC  0.87  0.87  0.87  0.85  0.84  0.88     0.85      0.82           8.00\n",
      "dili                               AUROC  0.93  0.92  0.92  0.91  0.90  0.58     0.96      0.87           7.00\n",
      "Scores                               NaN 17.91 17.23 16.68 15.81 14.81 14.82    16.55     11.84            NaN\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "hparams = {\n",
    "    \"dropout\": 0.5,\n",
    "    \"tasks\": tuple(trainer.task_losses.keys()), \n",
    "    \"head_params\": {\n",
    "        \"depth\": 3, \n",
    "        \"input_dim\": 256,\n",
    "        \"hidden_dim\": 256\n",
    "    },\n",
    "    \"trunk_params\": {\n",
    "        \"depth\": 4, \n",
    "        \"input_dim\": 512, \n",
    "        \"hidden_dim\": 512,\n",
    "        \"output_dim\": 256\n",
    "    }\n",
    "}\n",
    "lr              = 4e-3\n",
    "n_epochs        = 20\n",
    "n_warmup_epochs = 5\n",
    "batch_size      = 2048\n",
    "weight_decay    = 1e-4\n",
    "n_freeze_epochs = 5\n",
    "\n",
    "trainer = trainer_factory(batch_size)\n",
    "model   = MultiTaskModel(**hparams).to(device)\n",
    "\n",
    "optimizer        = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=lr / 1e2, total_iters=n_warmup_epochs)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs - n_warmup_epochs)\n",
    "scheduler        = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[n_warmup_epochs])\n",
    "\n",
    "trainer.train(model, optimizer, scheduler, num_epochs=n_epochs, n_freeze_epochs=n_freeze_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [0 / 20], Val loss: 1529.36\n",
    "Epoch [1 / 20], Train loss: 987.49, Val loss: 1515.75\n",
    "Epoch [2 / 20], Train loss: 478.12, Val loss: 997.80\n",
    "Epoch [3 / 20], Train loss: 531.30, Val loss: 996.84\n",
    "Epoch [4 / 20], Train loss: 440.09, Val loss: 944.69\n",
    "Epoch [5 / 20], Train loss: 451.10, Val loss: 995.97\n",
    "Epoch [6 / 20], Train loss: 493.83, Val loss: 958.22\n",
    "Epoch [7 / 20], Train loss: 326.13, Val loss: 950.37\n",
    "Epoch [8 / 20], Train loss: 319.23, Val loss: 902.01\n",
    "Epoch [9 / 20], Train loss: 502.29, Val loss: 853.17\n",
    "Epoch [10 / 20], Train loss: 384.61, Val loss: 883.08\n",
    "Epoch [11 / 20], Train loss: 280.79, Val loss: 788.70\n",
    "Epoch [12 / 20], Train loss: 209.52, Val loss: 767.53\n",
    "Epoch [13 / 20], Train loss: 175.38, Val loss: 763.10\n",
    "Epoch [14 / 20], Train loss: 239.95, Val loss: 702.57\n",
    "Epoch [15 / 20], Train loss: 168.21, Val loss: 688.34\n",
    "Epoch [16 / 20], Train loss: 111.77, Val loss: 711.55\n",
    "Epoch [17 / 20], Train loss: 132.02, Val loss: 673.70\n",
    "Epoch [18 / 20], Train loss: 106.15, Val loss: 673.86\n",
    "Epoch [19 / 20], Train loss: 117.40, Val loss: 675.25\n",
    "Epoch [20 / 20], Train loss: 117.32, Val loss: 676.77\n",
    "Epoch [20 / 20], Test loss: 186.12\n",
    "\n",
    "                                 metrics  tdc1  tdc2  tdc3  tdc4  tdc5  mole  minimol  this_run  this_run_rank\n",
    "caco2_wang                           MAE  0.28  0.28  0.29  0.29  0.29  0.31     0.35      0.38           8.00\n",
    "bioavailability_ma                 AUROC  0.75  0.74  0.73  0.71  0.67  0.65     0.69      0.64           8.00\n",
    "lipophilicity_astrazeneca            MAE  0.47  0.47  0.48  0.53  0.54  0.47     0.46      0.55           8.00\n",
    "solubility_aqsoldb                   MAE  0.76  0.78  0.79  0.79  0.83  0.79     0.74      1.08           8.00\n",
    "hia_hou                            AUROC  0.99  0.99  0.99  0.98  0.98  0.96     0.99      0.97           7.00\n",
    "pgp_broccatelli                    AUROC  0.94  0.94  0.93  0.93  0.93  0.92     0.94      0.93           7.00\n",
    "bbb_martins                        AUROC  0.92  0.92  0.91  0.91  0.91  0.90     0.92      0.87           8.00\n",
    "ppbr_az                              MAE  7.53  7.66  7.79  7.91  8.29  8.07     7.70      8.88           8.00\n",
    "vdss_lombardo                   Spearman  0.71  0.71  0.63  0.61  0.58  0.65     0.54      0.35           8.00\n",
    "cyp2c9_veith                       AUPRC  0.86  0.84  0.83  0.79  0.78  0.80     0.82      0.77           8.00\n",
    "cyp2d6_veith                       AUPRC  0.79  0.74  0.72  0.72  0.67  0.68     0.72      0.64           8.00\n",
    "cyp3a4_veith                       AUPRC  0.92  0.90  0.90  0.88  0.88  0.88     0.88      0.85           8.00\n",
    "cyp2c9_substrate_carbonmangels     AUPRC  0.47  0.44  0.44  0.43  0.41  0.45     0.47      0.50           1.00\n",
    "cyp2d6_substrate_carbonmangels     AUPRC  0.74  0.72  0.71  0.70  0.69  0.70     0.69      0.70           4.50\n",
    "cyp3a4_substrate_carbonmangels     AUROC  0.66  0.65  0.65  0.64  0.64  0.67     0.66      0.67           2.00\n",
    "half_life_obach                 Spearman  0.56  0.56  0.55  0.54  0.44  0.55     0.49      0.45           7.00\n",
    "clearance_hepatocyte_az         Spearman  0.50  0.47  0.44  0.44  0.43  0.38     0.45      0.48           2.00\n",
    "clearance_microsome_az          Spearman  0.63  0.63  0.62  0.60  0.60  0.61     0.63      0.56           8.00\n",
    "ld50_zhu                             MAE  0.55  0.59  0.61  0.62  0.62  0.82     0.58      0.66           7.00\n",
    "herg                               AUROC  0.88  0.87  0.87  0.86  0.84  0.81     0.85      0.79           8.00\n",
    "ames                               AUROC  0.87  0.87  0.87  0.85  0.84  0.88     0.85      0.80           8.00\n",
    "dili                               AUROC  0.93  0.92  0.92  0.91  0.90  0.58     0.96      0.91           6.00\n",
    "Scores                               NaN 17.93 17.32 16.83 15.98 15.17 15.06    16.60     12.46            NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "entity, project = \"blazejba-gc\", \"multitask_pretraining\"\n",
    "run = wandb.init(entity=entity, project=project)\n",
    "\n",
    "# Simulate logging metrics over epochs\n",
    "for epoch in range(10):\n",
    "    accuracy = 0.9 + epoch * 0.01\n",
    "    wandb.log({\"epoch\": epoch, \"accuracy\": accuracy})\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".minimol_p3-12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
