{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nethome-blazejb/minimol/minimol/model.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  predictor.load_state_dict(torch.load(state_dict_path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "featurizer = Minimol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import pandas as pd\n",
    "from datamol.mol import standardize_smiles\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "from torch.nn import MSELoss as mse_loss\n",
    "from torch.nn import BCEWithLogitsLoss as bce_loss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, samples, task_names):\n",
    "        self.samples = samples['embeddings'].tolist()\n",
    "        self.targets = samples[task_names].fillna(np.nan).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['embeddings'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "seed = 42\n",
    "batch_size = 128\n",
    "\n",
    "group = admet_group(path='admet_data/')\n",
    "\n",
    "num_mols = 0\n",
    "task_losses = {}\n",
    "test_dataloaders = {}\n",
    "validation_dataloaders = {}\n",
    "df = pd.DataFrame(columns=['smiles'])\n",
    "\n",
    "for dataset_i, dataset_name in enumerate(group.dataset_names):\n",
    "    print(f\"{dataset_i + 1} / {len(group.dataset_names)} - {dataset_name}\")\n",
    "    benchmark = group.get(dataset_name)\n",
    "    name = benchmark['name']\n",
    "    mols_test = benchmark['test']\n",
    "    \n",
    "    with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull): # suppress output\n",
    "        mols_train, mols_valid = group.get_train_valid_split(benchmark=name, split_type='default', seed=seed)\n",
    "        mols_test['embeddings'] = featurizer(list(mols_test['Drug']))\n",
    "        mols_valid['embeddings'] = featurizer(list(mols_valid['Drug']))\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        'smiles': mols_train['Drug'],\n",
    "        dataset_name: mols_train['Y']\n",
    "    })\n",
    "\n",
    "    num_mols += len(temp_df)\n",
    "    df = pd.merge(df, temp_df, on='smiles', how='outer')\n",
    "\n",
    "    task_losses[name] =  bce_loss() if len(mols_test['Y'].unique()) == 2 else mse_loss()\n",
    "    test_dataloaders[name] = DataLoader(AdmetDataset(mols_test), batch_size=batch_size, shuffle=False)\n",
    "    validation_dataloaders[name] = DataLoader(AdmetDataset(mols_valid), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "df['embeddings'] = featurizer(list(df['smiles']))\n",
    "print(f\"All mols: {num_mols}\")\n",
    "print(f\"Unique mols: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>caco2_wang</th>\n",
       "      <th>hia_hou</th>\n",
       "      <th>pgp_broccatelli</th>\n",
       "      <th>bioavailability_ma</th>\n",
       "      <th>lipophilicity_astrazeneca</th>\n",
       "      <th>solubility_aqsoldb</th>\n",
       "      <th>bbb_martins</th>\n",
       "      <th>ppbr_az</th>\n",
       "      <th>vdss_lombardo</th>\n",
       "      <th>...</th>\n",
       "      <th>cyp3a4_substrate_carbonmangels</th>\n",
       "      <th>cyp2c9_substrate_carbonmangels</th>\n",
       "      <th>half_life_obach</th>\n",
       "      <th>clearance_microsome_az</th>\n",
       "      <th>clearance_hepatocyte_az</th>\n",
       "      <th>herg</th>\n",
       "      <th>ames</th>\n",
       "      <th>dili</th>\n",
       "      <th>ld50_zhu</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNC1(c2ccccc2Cl)CCCCC1=O</td>\n",
       "      <td>-4.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.1916), tensor(0.3334), tensor(0.8102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNC1(c2ccccc2Cl)CCCCC1=O</td>\n",
       "      <td>-4.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.1916), tensor(0.3334), tensor(0.8102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C/C=C/C/C=C/CCC(=O)[C@@H]1O[C@@H]1C(N)=O</td>\n",
       "      <td>-5.422406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.2584), tensor(0.7886), tensor(1.3459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(NC1(C(=O)N[C@H](Cc2ccccc2)C(=O)NCCCC(=O)N2...</td>\n",
       "      <td>-5.769776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(1.8761), tensor(0.1187), tensor(1.0743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccccc1)NC...</td>\n",
       "      <td>-7.431799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tensor(2.0538), tensor(1.9126), tensor(1.1178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  caco2_wang  hia_hou  \\\n",
       "0                           CNC1(c2ccccc2Cl)CCCCC1=O   -4.260000      NaN   \n",
       "1                           CNC1(c2ccccc2Cl)CCCCC1=O   -4.260000      NaN   \n",
       "2           C/C=C/C/C=C/CCC(=O)[C@@H]1O[C@@H]1C(N)=O   -5.422406      NaN   \n",
       "3  O=C(NC1(C(=O)N[C@H](Cc2ccccc2)C(=O)NCCCC(=O)N2...   -5.769776      NaN   \n",
       "4  NC(=O)[C@H](Cc1ccccc1)NC(=O)[C@H](Cc1ccccc1)NC...   -7.431799      NaN   \n",
       "\n",
       "   pgp_broccatelli  bioavailability_ma  lipophilicity_astrazeneca  \\\n",
       "0              NaN                 0.0                        NaN   \n",
       "1              NaN                 0.0                        NaN   \n",
       "2              NaN                 NaN                        NaN   \n",
       "3              NaN                 NaN                        NaN   \n",
       "4              NaN                 NaN                        NaN   \n",
       "\n",
       "   solubility_aqsoldb  bbb_martins  ppbr_az  vdss_lombardo  ...  \\\n",
       "0                 NaN          1.0    44.84            NaN  ...   \n",
       "1                 NaN          1.0    42.01            NaN  ...   \n",
       "2                 NaN          NaN      NaN            NaN  ...   \n",
       "3                 NaN          NaN      NaN            NaN  ...   \n",
       "4                 NaN          NaN      NaN            NaN  ...   \n",
       "\n",
       "   cyp3a4_substrate_carbonmangels  cyp2c9_substrate_carbonmangels  \\\n",
       "0                             NaN                             NaN   \n",
       "1                             NaN                             NaN   \n",
       "2                             NaN                             NaN   \n",
       "3                             NaN                             NaN   \n",
       "4                             NaN                             NaN   \n",
       "\n",
       "   half_life_obach  clearance_microsome_az  clearance_hepatocyte_az  herg  \\\n",
       "0              NaN                     NaN                      NaN   NaN   \n",
       "1              NaN                     NaN                      NaN   NaN   \n",
       "2              NaN                     NaN                      NaN   NaN   \n",
       "3              NaN                     NaN                      NaN   NaN   \n",
       "4              NaN                     NaN                      NaN   NaN   \n",
       "\n",
       "   ames  dili  ld50_zhu                                         embeddings  \n",
       "0   NaN   0.0       NaN  [tensor(1.1916), tensor(0.3334), tensor(0.8102...  \n",
       "1   NaN   0.0       NaN  [tensor(1.1916), tensor(0.3334), tensor(0.8102...  \n",
       "2   NaN   NaN       NaN  [tensor(1.2584), tensor(0.7886), tensor(1.3459...  \n",
       "3   NaN   NaN       NaN  [tensor(1.8761), tensor(0.1187), tensor(1.0743...  \n",
       "4   NaN   NaN       NaN  [tensor(2.0538), tensor(1.9126), tensor(1.1178...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultitaskTrainer:\n",
    "    def __init__(self, device, batch_size: int = 256):\n",
    "        self.device = device\n",
    "        self.bs = batch_size\n",
    "        self.task_losses = {}\n",
    "        self.test_dataloaders = {}\n",
    "        self.validation_dataloaders = {}\n",
    "        self.train_dataloader = None\n",
    "    \n",
    "    def set_multitask_dataloader(self, data):\n",
    "        task_names = list(self.task_losses.keys())\n",
    "        self.train_dataloader = DataLoader(MultiDataset(data, task_names), batch_size=124, shuffle=True)\n",
    "\n",
    "    def set_per_task_dataloaders(self, test_loaders, val_loaders, task_losses):\n",
    "        self.task_losses = task_losses\n",
    "        self.test_dataloaders = test_loaders\n",
    "        self.validation_dataloaders = val_loaders\n",
    "\n",
    "    def _eval(self, model, dataloaders):\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        per_task_losses = torch.tensor([0.0]*len(dataloaders.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "        for task_i, (task_name, dataloader) in enumerate(dataloaders.items()):\n",
    "            for batch_idx, (samples, targets) in enumerate(dataloader):\n",
    "                samples = samples.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                output = model(samples, task=task_name)\n",
    "\n",
    "                task_loss = self.task_losses[task_name](output.float(), targets.float())\n",
    "                per_task_losses[task_i] = per_task_losses[task_i] + (task_loss / len(dataloader))\n",
    "                total_loss = total_loss + (task_loss / len(dataloader))\n",
    "\n",
    "        return total_loss / len(dataloaders.keys()), per_task_losses\n",
    "    \n",
    "    def eval_on_val(self, model):\n",
    "        return self._eval(model, self.validation_dataloaders)\n",
    "\n",
    "    def eval_on_test(self, model):\n",
    "        return self._eval(model, self.test_dataloaders)\n",
    "\n",
    "    def _compute_train_loss(self, outputs, filtered_targets):\n",
    "        total_loss = torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        per_task_losses = torch.tensor([0.0]*len(self.task_losses.keys()), requires_grad=False, device=self.device)\n",
    "\n",
    "        for task_i, (task, loss) in enumerate(self.task_losses.items()):\n",
    "            if task not in outputs.keys():\n",
    "                continue\n",
    "            \n",
    "            output = outputs[task]\n",
    "            target = filtered_targets[task]\n",
    "            assert not torch.isnan(output).any(), \"NaNs IN THE OUTPUT!!!\"\n",
    "\n",
    "            task_loss = loss(output.float(), target.float())\n",
    "            total_loss = total_loss + task_loss \n",
    "            per_task_losses[task_i] = task_loss\n",
    "\n",
    "        return total_loss / len(outputs.keys()), per_task_losses\n",
    "\n",
    "    def train(self, model, optimizer, num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_val_loss, per_task_val_loss = self._eval(model, self.validation_dataloaders)\n",
    "        print(f\"Epoch [0 / {num_epochs}], Val loss: {total_val_loss:.2f}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            running_per_task_loss = torch.tensor([0.0]*len(self.task_losses.keys()), requires_grad=False, device=self.device)\n",
    "            for batch_idx, (samples, targets) in enumerate(self.train_dataloader):\n",
    "                samples = samples.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                outputs, filtered_targets = model(samples, targets)\n",
    "                total_loss, per_task_losses = self._compute_train_loss(outputs, filtered_targets)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += total_loss.item()\n",
    "                running_per_task_loss += per_task_losses\n",
    "            \n",
    "            total_val_loss, per_task_val_loss = self._eval(model, self.validation_dataloaders)\n",
    "            print(f\"Epoch [{epoch + 1} / {num_epochs}], Train loss: {running_loss / len(self.train_dataloader):.2f}, Val loss: {total_val_loss:.2f}\")\n",
    "\n",
    "        total_test_loss, per_task_test_loss = self._eval(model, self.test_dataloaders)\n",
    "        print(f\"Epoch [{epoch + 1} / {num_epochs}], Test loss: {total_test_loss:.2f}\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trainer = MultitaskTrainer(device)\n",
    "trainer.set_per_task_dataloaders(test_dataloaders, validation_dataloaders, task_losses)\n",
    "trainer.set_multitask_dataloader(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size, head_input_size, hidden_size, depth, dropout):\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        for _ in range(depth - 1):\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, head_input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x += identity\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self, hidden_size, depth, dropout):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.LayerNorm(hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_size, trunk_hidden_size, trunk_depth, head_hidden_size, head_depth, dropout, tasks):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.trunk = ResidualMLP(input_size, head_hidden_size, trunk_hidden_size, trunk_depth, dropout)\n",
    "        self.heads = nn.ModuleDict({task: TaskHead(head_hidden_size, head_depth, dropout) for task in tasks})\n",
    "        self.tasks = list(tasks)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, targets=None, task=None):\n",
    "        x = self.trunk(x)\n",
    "        outputs = {}\n",
    "        filtered_targets = {}\n",
    "\n",
    "        if task:\n",
    "            return self.heads[task](x).squeeze()\n",
    "\n",
    "        task_mask = ~torch.isnan(targets)\n",
    "        for idx, task in enumerate(self.tasks): \n",
    "            if task_mask[:, idx].any():\n",
    "                indices = torch.nonzero(task_mask[:, idx], as_tuple=False).squeeze()\n",
    "                outputs[task] = self.heads[task](x[indices]).squeeze()\n",
    "                filtered_targets[task] = targets[indices, idx].squeeze()\n",
    "        return outputs, filtered_targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 10], Val loss: 1559.43\n",
      "Epoch [1 / 10], Train loss: 536.90, Val loss: 1085.34\n",
      "Epoch [2 / 10], Train loss: 506.49, Val loss: 1050.40\n",
      "Epoch [3 / 10], Train loss: 578.71, Val loss: 1055.92\n",
      "Epoch [4 / 10], Train loss: 410.20, Val loss: 1020.79\n",
      "Epoch [5 / 10], Train loss: 442.48, Val loss: 1016.70\n",
      "Epoch [6 / 10], Train loss: 313.46, Val loss: 1022.00\n",
      "Epoch [7 / 10], Train loss: 431.29, Val loss: 1041.65\n",
      "Epoch [8 / 10], Train loss: 353.79, Val loss: 1014.82\n",
      "Epoch [9 / 10], Train loss: 391.63, Val loss: 1026.25\n",
      "Epoch [10 / 10], Train loss: 419.40, Val loss: 1008.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss as bce_loss\n",
    "from torch.nn import MSELoss as mse_loss\n",
    "\n",
    "hparams = {\n",
    "    \"trunk_hidden_size\": 512,\n",
    "    \"head_hidden_size\": 128, \n",
    "    \"tasks\": trainer.task_losses.keys(), \n",
    "    \"input_size\": 512, \n",
    "    \"trunk_depth\": 2, \n",
    "    \"head_depth\": 3, \n",
    "    \"dropout\": 0.5\n",
    "}\n",
    "model = MultiTaskModel(**hparams).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=.9, weight_decay=1e-4)\n",
    "trainer.train(model, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "entity, project = \"blazejba-gc\", \"multitask_pretraining\"\n",
    "run = wandb.init(entity=entity, project=project)\n",
    "\n",
    "# Simulate logging metrics over epochs\n",
    "for epoch in range(10):\n",
    "    accuracy = 0.9 + epoch * 0.01\n",
    "    wandb.log({\"epoch\": epoch, \"accuracy\": accuracy})\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".minimol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
