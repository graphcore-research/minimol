{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream adaption with MiniMol\n",
    "\n",
    "This example shows how MiniMol can featurise small molecules that will then serve as an input to another model trained on a small downstream dataset from TDC ADMET. This allows to transfer the knowledge from the pre-trained MiniMol to another task. \n",
    "\n",
    "Before we start, let's make sure that the TDC package is installed in the environment. It takes a while, that's why we don't include it in the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyTDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Getting the data\n",
    "Next, we will build a predictor for the `HIA Hou` dataset, one of the binary classification benchmarks from TDC ADMET group. HIA stands for human intestinal absorption (HIA), which is related to the ability to absorb a substance through the gastrointestinal system into the bloodstream of the human body.\n",
    "\n",
    "We then split the data based on molecular scaffolds into training, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "generating training, validation splits...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3408.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "DATASET_NAME = 'HIA_Hou'\n",
    "\n",
    "admet = admet_group(path=\"admet-data/\")\n",
    "\n",
    "mols_test = admet.get(DATASET_NAME)['test']\n",
    "mols_train, mols_val = admet.get_train_valid_split(benchmark=DATASET_NAME, split_type='scaffold', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - HIA_Hou\n",
      "\n",
      "Val split (58 mols): \n",
      "                 Drug_ID                                               Drug  Y\n",
      "0         Atracurium.mol  COc1ccc(C[C@H]2c3cc(OC)c(OC)cc3CC[N@@+]2(C)CCC...  0\n",
      "1  Succinylsulfathiazole          O=C(O)CCC(=O)Nc1ccc(S(=O)(=O)Nc2nccs2)cc1  0\n",
      "2            Ticarcillin  CC1(C)S[C@H]2[C@@H](NC(=O)[C@@H](C(=O)O)c3ccsc...  0\n",
      "3          Raffinose.mol  OC[C@@H]1O[C@@H](OC[C@@H]2O[C@@H](O[C@]3(CO)O[...  0\n",
      "4          Triamcinolone  C[C@@]12C=CC(=O)C=C1CC[C@@H]1[C@H]3C[C@@H](O)[...  1\n",
      "\n",
      "Test split (117 mols): \n",
      "                Drug_ID                                               Drug  Y\n",
      "0         Trazodone.mol         O=c1n(CCCN2CCN(c3cccc(Cl)c3)CC2)nc2ccccn12  1\n",
      "1          Lisuride.mol  CCN(CC)C(=O)N[C@H]1C=C2c3cccc4[nH]cc(c34)C[C@@...  1\n",
      "2  Methylergonovine.mol  CC[C@H](CO)NC(=O)[C@H]1C=C2c3cccc4[nH]cc(c34)C...  1\n",
      "3      Methysergide.mol  CC[C@H](CO)NC(=O)[C@H]1C=C2c3cccc4c3c(cn4C)C[C...  1\n",
      "4       Moclobemide.mol                       O=C(NCCN1CCOCC1)c1ccc(Cl)cc1  1\n",
      "\n",
      "Train split (403 mols): \n",
      "           Drug_ID                                               Drug  Y\n",
      "0        Guanadrel                      N=C(N)NC[C@@H]1COC2(CCCCC2)O1  1\n",
      "1      Cefmetazole  CO[C@@]1(NC(=O)CSCC#N)C(=O)N2C(C(=O)O)=C(CSc3n...  0\n",
      "2   Zonisamide.mol                           NS(=O)(=O)Cc1noc2ccccc12  1\n",
      "3   Furosemide.mol            NS(=O)(=O)c1cc(Cl)cc(NCc2ccco2)c1C(=O)O  1\n",
      "4  Telmisartan.mol  CCCc1nc2c(n1Cc1ccc(-c3ccccc3C(=O)O)cc1)=C[C@H]...  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset - {DATASET_NAME}\\n\")\n",
    "print(f\"Val split ({len(mols_val)} mols): \\n{mols_val.head()}\\n\")\n",
    "print(f\"Test split ({len(mols_test)} mols): \\n{mols_test.head()}\\n\")\n",
    "print(f\"Train split ({len(mols_train)} mols): \\n{mols_train.head()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating molecular fingerprints\n",
    "Now that we have the splits, we will use MiniMol to embed all molecules. The embedding will be added as an extra column in the dataframe returned by TDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "featuriser = Minimol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "featurizing_smiles, batch=1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:05<00:00, 10.36it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 8542.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.75it/s]\n",
      "featurizing_smiles, batch=3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 384.38it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117/117 [00:00<00:00, 11496.90it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.36it/s]\n",
      "featurizing_smiles, batch=13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 119.64it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 18593.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.76it/s]\n"
     ]
    }
   ],
   "source": [
    "mols_val['Embedding'] = featuriser(list(mols_val['Drug']))\n",
    "mols_test['Embedding'] = featuriser(list(mols_test['Drug']))\n",
    "mols_train['Embedding'] = featuriser(list(mols_train['Drug']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is small, so it took us 6.6 seconds to generate the embeddings for almost 600 molecules. Here is a preview after the new column has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Drug_ID                                               Drug  Y  \\\n",
      "0        Guanadrel                      N=C(N)NC[C@@H]1COC2(CCCCC2)O1  1   \n",
      "1      Cefmetazole  CO[C@@]1(NC(=O)CSCC#N)C(=O)N2C(C(=O)O)=C(CSc3n...  0   \n",
      "2   Zonisamide.mol                           NS(=O)(=O)Cc1noc2ccccc12  1   \n",
      "3   Furosemide.mol            NS(=O)(=O)c1cc(Cl)cc(NCc2ccco2)c1C(=O)O  1   \n",
      "4  Telmisartan.mol  CCCc1nc2c(n1Cc1ccc(-c3ccccc3C(=O)O)cc1)=C[C@H]...  1   \n",
      "\n",
      "                                           Embedding  \n",
      "0  [0.0, 0.5511834, 0.8979431, 0.19619876, 0.7452...  \n",
      "1  [0.0, 0.0, 0.7467004, 0.17419952, 0.11563285, ...  \n",
      "2  [0.0, 1.0714495, 1.5657394, 0.0, 1.3655123, 0....  \n",
      "3  [0.0, 0.0, 0.40348637, 0.0, 0.0, 0.8121755, 0....  \n",
      "4  [0.08408047, 0.0, 0.0, 0.0, 0.0, 1.8257095, 0....  \n"
     ]
    }
   ],
   "source": [
    "print(mols_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training a model\n",
    "Now that the molecules are featurised leverging the representation MiniMol learned during its pre-training, we will set up a training and evaluation loop of a simple Multi-Layer Perceptron model using PyTorch.\n",
    "\n",
    "Let's start by defining a new class for the dataset and then creating a separate dataloader for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "    \n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['Embedding'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "val_loader = DataLoader(AdmetDataset(mols_val), batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(AdmetDataset(mols_test), batch_size=128, shuffle=False)\n",
    "train_loader = DataLoader(AdmetDataset(mols_train), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a simple 3-layer perceptron with batch normalisation and dropout. We also add a residual connection that before the last layer concatates the the input features with the output from the second to last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.dense1 = nn.Linear(512, 512)\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.dense3 = nn.Linear(512, 512)\n",
    "        self.final_dense = nn.Linear(1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_x = x\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.dense3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.cat((x, original_x), dim=1)\n",
    "        x = self.final_dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we declare the basic hyperparamters, optimiser, loss function and learning rate scheduler. We build a model factory that allows us to instatiate a fresh copy of everything, which will become useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "lr = 0.0003\n",
    "epochs = 25\n",
    "warmup = 5\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "def model_factory():\n",
    "    model = TaskHead()\n",
    "    optimiser = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "    lr_scheduler = CosineAnnealingLR(optimiser, T_max=epochs, eta_min=0)\n",
    "    return model, optimiser, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we will use both AUROC and Average Precision metrics. The reported loss would be an average across all samples in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate(predictor, dataloader, loss_fn):\n",
    "    predictor.eval()\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            probs = F.sigmoid(predictor(inputs).squeeze())\n",
    "            loss = loss_fn(probs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    loss = total_loss / len(all_probs)\n",
    "    \n",
    "    return (\n",
    "        loss,\n",
    "        roc_auc_score(all_targets, all_probs),\n",
    "        average_precision_score(all_targets, all_probs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is a rather standard boilerplate loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(predictor, train_loader, optimiser, lr_scheduler, loss_fn, epoch, verbose=True):\n",
    "    predictor.train()        \n",
    "    train_loss = 0\n",
    "    \n",
    "    lr_scheduler.step(epoch)\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimiser.zero_grad()\n",
    "        probs = torch.sigmoid(predictor(inputs).squeeze())\n",
    "        loss = loss_fn(probs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= (len(train_loader) * 32)\n",
    "\n",
    "    val_loss, auroc, avpr = evaluate(predictor, val_loader, loss_fn)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"## Epoch {epoch+1}\\t\"\n",
    "            f\"train_loss: {train_loss:.4f}\\t\"\n",
    "            f\"val_loss: {val_loss:.4f}\\t\"\n",
    "            f\"val_auroc: {auroc:.4f}\\t\"\n",
    "            f\"val_avpr: {avpr:.4f}\"\n",
    "        )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's see how good our model gets after training... ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 0\ttrain_loss: ------\tval_loss: 0.0116\tval_auroc: 0.4110\tval_avpr: 0.8551\n",
      "## Epoch 1\ttrain_loss: 0.0228\tval_loss: 0.0090\tval_auroc: 0.5313\tval_avpr: 0.9012\n",
      "## Epoch 2\ttrain_loss: 0.0119\tval_loss: 0.0062\tval_auroc: 0.7820\tval_avpr: 0.9634\n",
      "## Epoch 3\ttrain_loss: 0.0075\tval_loss: 0.0044\tval_auroc: 0.8722\tval_avpr: 0.9822\n",
      "## Epoch 4\ttrain_loss: 0.0049\tval_loss: 0.0037\tval_auroc: 0.8571\tval_avpr: 0.9784\n",
      "## Epoch 5\ttrain_loss: 0.0038\tval_loss: 0.0035\tval_auroc: 0.7970\tval_avpr: 0.9670\n",
      "## Epoch 6\ttrain_loss: 0.0027\tval_loss: 0.0034\tval_auroc: 0.8546\tval_avpr: 0.9769\n",
      "## Epoch 7\ttrain_loss: 0.0024\tval_loss: 0.0032\tval_auroc: 0.8571\tval_avpr: 0.9773\n",
      "## Epoch 8\ttrain_loss: 0.0020\tval_loss: 0.0034\tval_auroc: 0.8271\tval_avpr: 0.9732\n",
      "## Epoch 9\ttrain_loss: 0.0014\tval_loss: 0.0032\tval_auroc: 0.8521\tval_avpr: 0.9775\n",
      "## Epoch 10\ttrain_loss: 0.0015\tval_loss: 0.0032\tval_auroc: 0.8672\tval_avpr: 0.9794\n",
      "## Epoch 11\ttrain_loss: 0.0014\tval_loss: 0.0033\tval_auroc: 0.8496\tval_avpr: 0.9759\n",
      "## Epoch 12\ttrain_loss: 0.0014\tval_loss: 0.0033\tval_auroc: 0.8195\tval_avpr: 0.9706\n",
      "## Epoch 13\ttrain_loss: 0.0010\tval_loss: 0.0031\tval_auroc: 0.8622\tval_avpr: 0.9792\n",
      "## Epoch 14\ttrain_loss: 0.0009\tval_loss: 0.0031\tval_auroc: 0.8647\tval_avpr: 0.9795\n",
      "## Epoch 15\ttrain_loss: 0.0008\tval_loss: 0.0031\tval_auroc: 0.8672\tval_avpr: 0.9797\n",
      "## Epoch 16\ttrain_loss: 0.0010\tval_loss: 0.0031\tval_auroc: 0.8672\tval_avpr: 0.9799\n",
      "## Epoch 17\ttrain_loss: 0.0008\tval_loss: 0.0031\tval_auroc: 0.8622\tval_avpr: 0.9796\n",
      "## Epoch 18\ttrain_loss: 0.0008\tval_loss: 0.0031\tval_auroc: 0.8697\tval_avpr: 0.9806\n",
      "## Epoch 19\ttrain_loss: 0.0007\tval_loss: 0.0031\tval_auroc: 0.8697\tval_avpr: 0.9803\n",
      "## Epoch 20\ttrain_loss: 0.0008\tval_loss: 0.0032\tval_auroc: 0.8697\tval_avpr: 0.9804\n",
      "## Epoch 21\ttrain_loss: 0.0007\tval_loss: 0.0031\tval_auroc: 0.8672\tval_avpr: 0.9799\n",
      "## Epoch 22\ttrain_loss: 0.0007\tval_loss: 0.0031\tval_auroc: 0.8647\tval_avpr: 0.9795\n",
      "## Epoch 23\ttrain_loss: 0.0007\tval_loss: 0.0031\tval_auroc: 0.8672\tval_avpr: 0.9799\n",
      "## Epoch 24\ttrain_loss: 0.0007\tval_loss: 0.0031\tval_auroc: 0.8672\tval_avpr: 0.9800\n",
      "## Epoch 25\ttrain_loss: 0.0008\tval_loss: 0.0031\tval_auroc: 0.8622\tval_avpr: 0.9792\n",
      "test_loss: 0.0017\n",
      "test_auroc: 0.9683\n",
      "test_avpr: 0.9906\n"
     ]
    }
   ],
   "source": [
    "model, optimiser, lr_scheduler = model_factory()\n",
    "\n",
    "val_loss, val_auroc, val_avpr = evaluate(model, val_loader, loss_fn)\n",
    "print(\n",
    "    f\"## Epoch 0\\t\"\n",
    "    f\"train_loss: ------\\t\"\n",
    "    f\"val_loss: {val_loss:.4f}\\t\"\n",
    "    f\"val_auroc: {val_auroc:.4f}\\t\"\n",
    "    f\"val_avpr: {val_avpr:.4f}\"\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model = train_one_epoch(model, train_loader, optimiser, lr_scheduler, loss_fn, epoch)\n",
    "\n",
    "test_loss, test_auroc, test_avpr = evaluate(model, test_loader, loss_fn)\n",
    "print(\n",
    "    f\"test_loss: {test_loss:.4f}\\n\"\n",
    "    f\"test_auroc: {test_auroc:.4f}\\n\"\n",
    "    f\"test_avpr: {test_avpr:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained in just 1.4s, reaching AUROC on the test set of 0.9683. Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Improvements\n",
    "\n",
    "The result can be further improved. One problem is that the accuracy is quite sensitive to both the train-val splitting (reminder - we use scaffold splitting strategy) and the weight initialisation. Let's visualise the distribution by training a few models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_factory(seed):\n",
    "    mols_train, mols_val = admet.get_train_valid_split(benchmark=DATASET_NAME, split_type='scaffold', seed=seed)\n",
    "\n",
    "    mols_val['Embedding'] = featuriser(list(mols_val['Drug']))\n",
    "    mols_train['Embedding'] = featuriser(list(mols_train['Drug']))\n",
    "\n",
    "    val_loader = DataLoader(AdmetDataset(mols_val), batch_size=128, shuffle=False)\n",
    "    train_loader = DataLoader(AdmetDataset(mols_train), batch_size=32, shuffle=True)\n",
    "\n",
    "    return val_loader, train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3646.92it/s]\n",
      "featurizing_smiles, batch=1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 774.12it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 13845.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.81it/s]\n",
      "featurizing_smiles, batch=13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 118.93it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 14905.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.19it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA520lEQVR4nO3deXgUVd7+/7tJSIclAQRCiECCgbATQRQRFBAQ2UQFBSZo2HTGCQKuLD6KETE4MyAgCooQdtkXH5B983Fh2ARZZJdNAkGEhIAESM7vD3/01yYJS9OdqsD7dV19zdTpU1WfOimS21NV3Q5jjBEAAIAN5bO6AAAAgJwQVAAAgG0RVAAAgG0RVAAAgG0RVAAAgG0RVAAAgG0RVAAAgG0RVAAAgG0RVAAAgG0RVHBHeffdd+VwOHJlX40aNVKjRo1cy2vWrJHD4dDs2bNzZf9dunRRREREruzLU2lpaerRo4dCQ0PlcDjUp08fq0vCTYiIiFCXLl2sLgO3OYIK8qwJEybI4XC4XoGBgQoLC1Pz5s01cuRInT171iv7OXbsmN59911t2bLFK9vzJjvXdiM++OADTZgwQS+99JImT56s55577rrrZGRkKCwsTA6HQ4sXL862T5cuXVS4cOEct1G4cGG3P7BXQuSVl5+fn0JCQtS+fXv9/PPPOW5n4cKFevzxx1W8eHEFBgYqKipKr7/+uk6dOpXjOmvWrNHTTz+t0NBQBQQEKCQkRG3atNHcuXOve+x2t3PnTr377rs6ePCg1aXgNkJQQZ733nvvafLkyRo9erRefvllSVKfPn1Uo0YN/fTTT259/+d//kd//PHHTW3/2LFjio+Pv+kwsGzZMi1btuym1rlZ16pt7Nix2r17t0/3f6tWrVqlBx98UAMHDlTnzp1133333dA6SUlJioiI0NSpU71aT69evTR58mR98cUXiomJ0aJFi/Twww/r+PHjWfq+/vrratOmjY4fP66+fftq1KhRatq0qUaNGqXo6Ohsx37gwIFq3Lixtm/frr///e8aM2aM3njjDaWlpaldu3aaNm2aV48nt+3cuVPx8fEEFXiVv9UFALeqRYsWqlOnjmu5f//+WrVqlVq3bq0nnnhCP//8swoUKCBJ8vf3l7+/b0/78+fPq2DBggoICPDpfq4nf/78lu7/RiQnJ6tq1ao3tc6UKVNUu3ZtxcbGasCAATp37pwKFSrklXoefvhhtW/f3rVcqVIlvfTSS5o0aZLefPNNV/uXX36poUOHqkOHDpo6dar8/Pxc73Xp0kWNGzfWM888o82bN7vOt9mzZ+u9995T+/btNW3aNLefzxtvvKGlS5fq0qVLN1VvZmamLl68qMDAQE8PGbA9ZlRwW3r00Uf19ttv69ChQ5oyZYqrPbt7VJYvX64GDRqoaNGiKly4sCpVqqQBAwZI+nOa/v7775ckde3a1XVpYMKECZL+vA+levXq2rRpkx555BEVLFjQte7V96hckZGRoQEDBig0NFSFChXSE088oSNHjrj1yena/1+3eb3asrtH5dy5c3rttddUtmxZOZ1OVapUSf/5z3909ZeoOxwO9ezZU/Pnz1f16tXldDpVrVo1LVmyJPsBv0pycrK6d++uUqVKKTAwUNHR0Zo4caLr/SuXWn755RctWrTIVfv1/kv8jz/+0Lx589SxY0c9++yz+uOPP7RgwYIbqskTDz/8sCRp//79bu3x8fEqVqyYPv/8c7eQIkkPPPCA+vbtq23btrndj/T222/rrrvu0vjx47MNkc2bN1fr1q2vWc+Vn8vUqVNVrVo1OZ1O18/k119/Vbdu3VSqVCnXz2v8+PFZtvHxxx+rWrVqKliwoIoVK6Y6deq4zeTkdG/T9e7vmjBhgp555hlJUuPGjV0/0zVr1kiSNm7cqObNm6tEiRIqUKCAypcvr27dul3zeAGJGRXcxp577jkNGDBAy5Yt0wsvvJBtnx07dqh169aqWbOm3nvvPTmdTu3bt0/fffedJKlKlSp677339M477+jFF190/eF66KGHXNs4deqUWrRooY4dO6pz584qVarUNesaPHiwHA6H+vbtq+TkZA0fPlxNmzbVli1bXDM/N+JGavsrY4yeeOIJrV69Wt27d9e9996rpUuX6o033tCvv/6qjz76yK3/t99+q7lz5+qf//yngoKCNHLkSLVr106HDx9W8eLFc6zrjz/+UKNGjbRv3z717NlT5cuX16xZs9SlSxedOXNGvXv3VpUqVTR58mS98sorKlOmjF577TVJUsmSJa95zF999ZXS0tLUsWNHhYaGqlGjRpo6dar+9re/3fC43YwrwalYsWKutr1792r37t3q0qWLgoODs13v+eef18CBA7Vw4UJ17NhRe/fu1a5du9StWzcFBQXdUk2rVq3SzJkz1bNnT5UoUUIRERE6ceKEHnzwQVeQKVmypBYvXqzu3bsrNTXVdZPy2LFj1atXL7Vv3169e/fWhQsX9NNPP+m///3vLY/hI488ol69emnkyJEaMGCAqlSpIunP8zQ5OVmPPfaYSpYsqX79+qlo0aI6ePDgbXFfDnKBAfKoxMREI8ls2LAhxz5FihQxtWrVci0PHDjQ/PW0/+ijj4wkc/LkyRy3sWHDBiPJJCYmZnmvYcOGRpIZM2ZMtu81bNjQtbx69Wojydx9990mNTXV1T5z5kwjyYwYMcLVFh4ebmJjY6+7zWvVFhsba8LDw13L8+fPN5LM+++/79avffv2xuFwmH379rnaJJmAgAC3tq1btxpJ5uOPP86yr78aPny4kWSmTJniart48aKpV6+eKVy4sNuxh4eHm1atWl1ze3/VunVrU79+fdfy559/bvz9/U1ycrJbv9jYWFOoUKEct1OoUCG38b3ysxk/frw5efKkOXbsmFmyZImpUKGCcTgcZv369a6+V8bxo48+umatwcHBpnbt2sYYYxYsWHBD61yPJJMvXz6zY8cOt/bu3bub0qVLm99++82tvWPHjqZIkSLm/Pnzxhhj2rZta6pVq3bNfVx93lxx9b8dY7Kep7NmzTKSzOrVq936zZs377r/VoGccOkHt7XChQtf8+mfokWLSpIWLFigzMxMj/bhdDrVtWvXG+7//PPPu/1Xdfv27VW6dGl9/fXXHu3/Rn399dfy8/NTr1693Npfe+01GWOyPEHTtGlTRUZGupZr1qyp4OBgHThw4Lr7CQ0NVadOnVxt+fPnV69evZSWlqa1a9d6VP+pU6e0dOlSt+22a9dODodDM2fO9GibV+vWrZtKliypsLAwPf7440pJSdHkyZNdl9gkuc6n682MBAUFKTU1VZJc/3ursymS1LBhQ7f7eowxmjNnjtq0aSNjjH777TfXq3nz5kpJSdHmzZsl/Xm+Hz16VBs2bLjlOm7GlX9nCxcuvOn7cACCCm5raWlp1/zj0KFDB9WvX189evRQqVKl1LFjR82cOfOmQsvdd999UzfOVqxY0W3Z4XCoQoUKPn9S4tChQwoLC8syHlem6A8dOuTWXq5cuSzbKFasmE6fPn3d/VSsWFH58rn/eslpPzdqxowZunTpkmrVqqV9+/Zp3759+v3331W3bl2Pnv7J7n6Ld955R8uXL9e8efP0/PPPKyUlJctxXBm/6z3+fvbsWVffK5eIvPHIfPny5d2WT548qTNnzujzzz9XyZIl3V5XAnRycrIkqW/fvipcuLAeeOABVaxYUXFxca7LnL7UsGFDtWvXTvHx8SpRooTatm2rxMREpaen+3zfyPu4RwW3raNHjyolJUUVKlTIsU+BAgX0zTffaPXq1Vq0aJGWLFmiGTNm6NFHH9WyZcuy3CiZ0za8LaebFjMyMm6oJm/IaT/mqhtvc8uVMFK/fv1s3z9w4IDuueceSVJgYKDS09NljMkylsYYXbhwIdsnZWrUqKGmTZtKkp588kmdP39eL7zwgho0aKCyZctK+n+B6+pH3//q0KFDSk1Ndc18VK5cWZK0bdu2Gz7enFx9vl0J1Z07d1ZsbGy269SsWVPSn7Xv3r1bCxcu1JIlSzRnzhx9+umneueddxQfHy/p2ueep6580OG6dev0v//7v1q6dKm6deumoUOHat26ddf8zBuAGRXctiZPnizpz6cpriVfvnxq0qSJhg0bpp07d2rw4MFatWqVVq9eLSnnX9ye2rt3r9uyMUb79u1ze9KiWLFiOnPmTJZ1r56NuJnawsPDdezYsSz/Vb9r1y7X+94QHh6uvXv3ZpmVupX9/PLLL/r+++/Vs2dPzZo1y+01Y8YMBQQEuD25Eh4ersuXL2d5WkeS9u3bp4yMjBuqY8iQIbpw4YIGDx7saouKilJUVJTmz5+f4wzJpEmTJMn1FE9UVJQqVaqkBQsWKC0t7aaO/XpKliypoKAgZWRkqGnTptm+QkJCXP0LFSqkDh06KDExUYcPH1arVq00ePBgXbhwQdKNn3vZud75+OCDD2rw4MHauHGjpk6dqh07dmj69Ok3d8C44xBUcFtatWqVBg0apPLlyysmJibHfr///nuWtnvvvVeSXNPSVz6jI7tf3p6YNGmS2x+42bNnKykpSS1atHC1RUZGat26dbp48aKrbeHChVkeY76Z2lq2bKmMjAyNGjXKrf2jjz6Sw+Fw2/+taNmypY4fP64ZM2a42i5fvqyPP/5YhQsXVsOGDW96m1dmU9588021b9/e7fXss8+qYcOGbpd/rhzL1ccqSZ988olbn2uJjIxUu3btNGHCBLcPfXvnnXd0+vRp/eMf/8gy07Bp0yZ9+OGHql69utq1a+dqj4+P16lTp9SjRw9dvnw5y76WLVumhQsXXremq/n5+aldu3aaM2eOtm/fnuX9kydPuv7/1Z+YGxAQoKpVq8oY47p3JDIyUikpKW4zRklJSZo3b951a8npfDx9+nSWmbir/50BOeHSD/K8xYsXa9euXbp8+bJOnDihVatWafny5QoPD9dXX311zQ/Deu+99/TNN9+oVatWCg8PV3Jysj799FOVKVNGDRo0kPTnL+6iRYtqzJgxCgoKUqFChVS3bt0s9wrcqLvuuksNGjRQ165ddeLECQ0fPlwVKlRwe4S6R48emj17th5//HE9++yz2r9/v6ZMmeJ2c+vN1tamTRs1btxYb731lg4ePKjo6GgtW7ZMCxYsUJ8+fbJs21MvvviiPvvsM3Xp0kWbNm1SRESEZs+ere+++07Dhw/36IbSqVOn6t5773VdfrnaE088oZdfflmbN29W7dq1de+996pHjx4aMWKE9u7dq2bNmkn68zNzvv76a/Xo0UPR0dE3tO833nhDM2fO1PDhwzVkyBBJUkxMjDZs2KARI0Zo586diomJUbFixbR582aNHz9exYsX1+zZs90+L6VDhw7atm2bBg8erB9//FGdOnVSeHi4Tp06pSVLlmjlypUefzLtkCFDtHr1atWtW1cvvPCCqlatqt9//12bN2/WihUrXIH8scceU2hoqOrXr69SpUrp559/1qhRo9SqVSvXz6Vjx47q27evnnrqKfXq1Uvnz5/X6NGjFRUV5bopNyf33nuv/Pz89OGHHyolJUVOp1OPPvqopk2bpk8//VRPPfWUIiMjdfbsWY0dO1bBwcFq2bKlR8eMO4hlzxsBt+jK48lXXgEBASY0NNQ0a9bMjBgxwu0x2CuufsRy5cqVpm3btiYsLMwEBASYsLAw06lTJ7Nnzx639RYsWGCqVq1q/P393R4HbtiwYY6Pe+b0ePKXX35p+vfvb0JCQkyBAgVMq1atzKFDh7KsP3ToUHP33Xcbp9Np6tevbzZu3Jhlm9eqLbvHTM+ePWteeeUVExYWZvLnz28qVqxo/v3vf5vMzEy3fpJMXFxclppyemz6aidOnDBdu3Y1JUqUMAEBAaZGjRrZPkJ9I48nb9q0yUgyb7/9do59Dh48aCSZV155xdWWkZFhRowYYaKjo01gYKAJDAw00dHRZuTIkSYjI8Nt/Ss/m1mzZmW7/UaNGpng4GBz5swZt/b58+ebZs2amWLFihmn02kqVKhgXnvttWs+7n7lnAsJCTH+/v6mZMmSpk2bNmbBggXXHAdjcv65GPPnmMfFxZmyZcua/Pnzm9DQUNOkSRPz+eefu/p89tln5pFHHjHFixc3TqfTREZGmjfeeMOkpKS4bWvZsmWmevXqJiAgwFSqVMlMmTLlhh5PNsaYsWPHmnvuucf4+fm5HlXevHmz6dSpkylXrpxxOp0mJCTEtG7d2mzcuPG6xww4jLHozjgAAIDr4B4VAABgWwQVAABgWwQVAABgWwQVAABgWwQVAABgWwQVAABgW3n6A98yMzN17NgxBQUFef1jzgEAgG8YY3T27FmFhYVl+eLPq+XpoHLs2LEcP6kSAADY25EjR1SmTJlr9snTQeXKRz4fOXLE9TXqAADA3lJTU1W2bNkb+kqNPB1UrlzuCQ4OJqgAAJDH3MhtG9xMCwAAbIugAgAAbIugAgAAbIugAgAAbIugAgAAbIugAgAAbIugAgAAbIugAgAAbIugAgAAbIugAgAAbMvSoJKRkaG3335b5cuXV4ECBRQZGalBgwbJGGNlWQAAwCYs/a6fDz/8UKNHj9bEiRNVrVo1bdy4UV27dlWRIkXUq1cvK0sDAAA2YGlQ+f7779W2bVu1atVKkhQREaEvv/xS69evt7IsAABgE5Ze+nnooYe0cuVK7dmzR5K0detWffvtt2rRooWVZQEAAJuwdEalX79+Sk1NVeXKleXn56eMjAwNHjxYMTEx2fZPT09Xenq6azk1NTW3SgUAABawNKjMnDlTU6dO1bRp01StWjVt2bJFffr0UVhYmGJjY7P0T0hIUHx8vAWVAsD/E9FvkdUlZHFwSCurSwB8wmEsfMSmbNmy6tevn+Li4lxt77//vqZMmaJdu3Zl6Z/djErZsmWVkpKi4ODgXKkZAAgqwK1JTU1VkSJFbujvt6UzKufPn1e+fO63yfj5+SkzMzPb/k6nU06nMzdKAwAANmBpUGnTpo0GDx6scuXKqVq1avrxxx81bNgwdevWzcqyAACATVgaVD7++GO9/fbb+uc//6nk5GSFhYXp73//u9555x0rywIAADZhaVAJCgrS8OHDNXz4cCvLAAAANsV3/QAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANuyNKhERETI4XBkecXFxVlZFgAAsAl/K3e+YcMGZWRkuJa3b9+uZs2a6ZlnnrGwKgAAYBeWBpWSJUu6LQ8ZMkSRkZFq2LChRRUBAAA7sc09KhcvXtSUKVPUrVs3ORwOq8sBAAA2YOmMyl/Nnz9fZ86cUZcuXXLsk56ervT0dNdyampqLlQGAACsYpugMm7cOLVo0UJhYWE59klISFB8fHwuVgUAeUNEv0VWl5Ctg0NaWV1CFoxV3mKLSz+HDh3SihUr1KNHj2v269+/v1JSUlyvI0eO5FKFAADACraYUUlMTFRISIhatbp2mnQ6nXI6nblUFQAAsJrlMyqZmZlKTExUbGys/P1tkZsAAIBNWB5UVqxYocOHD6tbt25WlwIAAGzG8imMxx57TMYYq8sAAAA2ZPmMCgAAQE4IKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYsDyq//vqrOnfurOLFi6tAgQKqUaOGNm7caHVZAADABvyt3Pnp06dVv359NW7cWIsXL1bJkiW1d+9eFStWzMqyAACATVgaVD788EOVLVtWiYmJrrby5ctbWBEAALATSy/9fPXVV6pTp46eeeYZhYSEqFatWho7dqyVJQEAABuxNKgcOHBAo0ePVsWKFbV06VK99NJL6tWrlyZOnJht//T0dKWmprq9AADA7cvSSz+ZmZmqU6eOPvjgA0lSrVq1tH37do0ZM0axsbFZ+ickJCg+Pj63ywQAABaxdEaldOnSqlq1qltblSpVdPjw4Wz79+/fXykpKa7XkSNHcqNMAABgEUtnVOrXr6/du3e7te3Zs0fh4eHZ9nc6nXI6nblRGgAAsAFLZ1ReeeUVrVu3Th988IH27dunadOm6fPPP1dcXJyVZQEAAJuwNKjcf//9mjdvnr788ktVr15dgwYN0vDhwxUTE2NlWQAAwCYsvfQjSa1bt1br1q2tLgMAANiQ5R+hDwAAkBOCCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC1Lg8q7774rh8Ph9qpcubKVJQEAABvxt7qAatWqacWKFa5lf3/LSwIAADZheSrw9/dXaGio1WUAAAAbsvwelb179yosLEz33HOPYmJidPjwYatLAgAANmHpjErdunU1YcIEVapUSUlJSYqPj9fDDz+s7du3KygoKEv/9PR0paenu5ZTU1Nzs1wAAJDLLA0qLVq0cP3/mjVrqm7dugoPD9fMmTPVvXv3LP0TEhIUHx+fmyUCd4yIfousLiGLg0NaWV0CbpEdzyvkLZZf+vmrokWLKioqSvv27cv2/f79+yslJcX1OnLkSC5XCAAAcpNHQeXAgQPerkOSlJaWpv3796t06dLZvu90OhUcHOz2AgAAty+PgkqFChXUuHFjTZkyRRcuXPB456+//rrWrl2rgwcP6vvvv9dTTz0lPz8/derUyeNtAgCA24dHQWXz5s2qWbOmXn31VYWGhurvf/+71q9ff9PbOXr0qDp16qRKlSrp2WefVfHixbVu3TqVLFnSk7IAAMBtxqOgcu+992rEiBE6duyYxo8fr6SkJDVo0EDVq1fXsGHDdPLkyRvazvTp03Xs2DGlp6fr6NGjmj59uiIjIz0pCQAA3IZu6WZaf39/Pf3005o1a5Y+/PBD7du3T6+//rrKli2r559/XklJSd6qEwAA3IFuKahs3LhR//znP1W6dGkNGzZMr7/+uvbv36/ly5fr2LFjatu2rbfqBAAAdyCPPkdl2LBhSkxM1O7du9WyZUtNmjRJLVu2VL58f+ae8uXLa8KECYqIiPBmrQAA4A7jUVAZPXq0unXrpi5duuT4KHFISIjGjRt3S8UBAIA7m0dBZe/evdftExAQoNjYWE82DwAAIMnDe1QSExM1a9asLO2zZs3SxIkTb7koAAAAycOgkpCQoBIlSmRpDwkJ0QcffHDLRQEAAEgeBpXDhw+rfPnyWdrDw8N1+PDhWy4KAABA8jCohISE6KeffsrSvnXrVhUvXvyWiwIAAJA8DCqdOnVSr169tHr1amVkZCgjI0OrVq1S79691bFjR2/XCAAA7lAePfUzaNAgHTx4UE2aNJG//5+byMzM1PPPP889KgAAwGs8CioBAQGaMWOGBg0apK1bt6pAgQKqUaOGwsPDvV0fAAC4g3kUVK6IiopSVFSUt2oBAABw41FQycjI0IQJE7Ry5UolJycrMzPT7f1Vq1Z5pTgAAHBn8yio9O7dWxMmTFCrVq1UvXp1ORwOb9cFAADgWVCZPn26Zs6cqZYtW3q7HgAAABePHk8OCAhQhQoVvF0LAACAG4+CymuvvaYRI0bIGOPtegAAAFw8uvTz7bffavXq1Vq8eLGqVaum/Pnzu70/d+5crxQHAADubB4FlaJFi+qpp57ydi0AAABuPAoqiYmJ3q4DAAAgC4/uUZGky5cva8WKFfrss8909uxZSdKxY8eUlpbmteIAAMCdzaMZlUOHDunxxx/X4cOHlZ6ermbNmikoKEgffvih0tPTNWbMGG/XCQAA7kAezaj07t1bderU0enTp1WgQAFX+1NPPaWVK1d6rTgAAHBn82hG5f/+7//0/fffKyAgwK09IiJCv/76q1cKAwAA8GhGJTMzUxkZGVnajx49qqCgoFsuCgAAQPIwqDz22GMaPny4a9nhcCgtLU0DBw7kY/UBAIDXeHTpZ+jQoWrevLmqVq2qCxcu6G9/+5v27t2rEiVK6Msvv/R2jQAA4A7lUVApU6aMtm7dqunTp+unn35SWlqaunfvrpiYGLebawEAAG6FR0FFkvz9/dW5c2dv1gIAAODGo6AyadKka77//PPPe1QMAADAX3kUVHr37u22fOnSJZ0/f14BAQEqWLAgQQUAAHiFR0/9nD592u2Vlpam3bt3q0GDBtxMCwAAvMbj7/q5WsWKFTVkyJAssy03asiQIXI4HOrTp4+3SgIAAHmc14KK9OcNtseOHbvp9TZs2KDPPvtMNWvW9GY5AAAgj/PoHpWvvvrKbdkYo6SkJI0aNUr169e/qW2lpaUpJiZGY8eO1fvvv+9JOQAA4DblUVB58skn3ZYdDodKliypRx99VEOHDr2pbcXFxalVq1Zq2rQpQQUAALjxKKhkZmZ6ZefTp0/X5s2btWHDhhvqn56ervT0dNdyamqqV+oAAAD25PEHvt2qI0eOqHfv3lq+fLkCAwNvaJ2EhATFx8f7uDLcLiL6LbK6BADALfIoqLz66qs33HfYsGHZtm/atEnJycmqXbu2qy0jI0PffPONRo0apfT0dPn5+bmt079/f7d9p6amqmzZsjdZPQAAyCs8Cio//vijfvzxR126dEmVKlWSJO3Zs0d+fn5uwcPhcOS4jSZNmmjbtm1ubV27dlXlypXVt2/fLCFFkpxOp5xOpyclAwCAPMijoNKmTRsFBQVp4sSJKlasmKQ/PwSua9euevjhh/Xaa69ddxtBQUGqXr26W1uhQoVUvHjxLO0AAODO5NHnqAwdOlQJCQmukCJJxYoV0/vvv3/TT/0AAADkxKMZldTUVJ08eTJL+8mTJ3X27FmPi1mzZo3H6wIAgNuPRzMqTz31lLp27aq5c+fq6NGjOnr0qObMmaPu3bvr6aef9naNAADgDuXRjMqYMWP0+uuv629/+5suXbr054b8/dW9e3f9+9//9mqBAADgzuVRUClYsKA+/fRT/fvf/9b+/fslSZGRkSpUqJBXiwMAAHe2W/pSwqSkJCUlJalixYoqVKiQjDHeqgsAAMCzoHLq1Ck1adJEUVFRatmypZKSkiRJ3bt3v6FHkwEAAG6ER0HllVdeUf78+XX48GEVLFjQ1d6hQwctWbLEa8UBAIA7m0f3qCxbtkxLly5VmTJl3NorVqyoQ4cOeaUwAAAAj2ZUzp075zaTcsXvv//OR9wDAACv8SioPPzww5o0aZJr2eFwKDMzU//617/UuHFjrxUHAADubB5d+vnXv/6lJk2aaOPGjbp48aLefPNN7dixQ7///ru+++47b9cIAADuUB7NqFSvXl179uxRgwYN1LZtW507d05PP/20fvzxR0VGRnq7RgAAcIe66RmVS5cu6fHHH9eYMWP01ltv+aImAAAASR7MqOTPn18//fSTL2oBAABw49Gln86dO2vcuHHergUAAMCNRzfTXr58WePHj9eKFSt03333ZfmOn2HDhnmlOAAAcGe7qaBy4MABRUREaPv27apdu7Ykac+ePW59HA6H96oDAAB3tJsKKhUrVlRSUpJWr14t6c+PzB85cqRKlSrlk+IAAMCd7abuUbn625EXL16sc+fOebUgAACAKzy6mfaKq4MLAACAN91UUHE4HFnuQeGeFAAA4Cs3dY+KMUZdunRxffHghQsX9I9//CPLUz9z5871XoUAAOCOdVNBJTY21m25c+fOXi0GAADgr24qqCQmJvqqDgAAgCxu6WZaAAAAXyKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA27I0qIwePVo1a9ZUcHCwgoODVa9ePS1evNjKkgAAgI1YGlTKlCmjIUOGaNOmTdq4caMeffRRtW3bVjt27LCyLAAAYBM39aWE3tamTRu35cGDB2v06NFat26dqlWrZlFVAADALiwNKn+VkZGhWbNm6dy5c6pXr57V5QAAABuwPKhs27ZN9erV04ULF1S4cGHNmzdPVatWzbZvenq60tPTXcupqam5VSYAALCAwxhjrCzg4sWLOnz4sFJSUjR79mx98cUXWrt2bbZh5d1331V8fHyW9pSUFAUHB+dGuchBRL9FVpcAAPCBg0NaeX2bqampKlKkyA39/bY8qFytadOmioyM1GeffZblvexmVMqWLUtQsQGCCgDcnqwOKpZf+rlaZmamWxj5K6fTKafTmcsVAQAAq1gaVPr3768WLVqoXLlyOnv2rKZNm6Y1a9Zo6dKlVpYFAABswtKgkpycrOeff15JSUkqUqSIatasqaVLl6pZs2ZWlgUAAGzC0qAybtw4K3cPAABsju/6AQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtkVQAQAAtmVpUElISND999+voKAghYSE6Mknn9Tu3butLAkAANiIpUFl7dq1iouL07p167R8+XJdunRJjz32mM6dO2dlWQAAwCb8rdz5kiVL3JYnTJigkJAQbdq0SY888ohFVQEAALuw1T0qKSkpkqS77rrL4koAAIAdWDqj8leZmZnq06eP6tevr+rVq2fbJz09Xenp6a7l1NTU3CoPAABYwDYzKnFxcdq+fbumT5+eY5+EhAQVKVLE9SpbtmwuVggAAHKbLYJKz549tXDhQq1evVplypTJsV///v2VkpLieh05ciQXqwQAALnN0ks/xhi9/PLLmjdvntasWaPy5ctfs7/T6ZTT6cyl6gAAgNUsDSpxcXGaNm2aFixYoKCgIB0/flySVKRIERUoUMDK0gAAgA1Yeuln9OjRSklJUaNGjVS6dGnXa8aMGVaWBQAAbMLySz8AAAA5scXNtAAAANkhqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANuyNKh88803atOmjcLCwuRwODR//nwrywEAADZjaVA5d+6coqOj9cknn1hZBgAAsCl/K3feokULtWjRwsoSAACAjXGPCgAAsC1LZ1RuVnp6utLT013LqampFlYDAAB8LU8FlYSEBMXHx+fa/iL6Lcq1fd2og0NaWV0CAAC5Jk9d+unfv79SUlJcryNHjlhdEgAA8KE8NaPidDrldDqtLgMAAOQSS4NKWlqa9u3b51r+5ZdftGXLFt11110qV66chZUBAAA7sDSobNy4UY0bN3Ytv/rqq5Kk2NhYTZgwwaKqAACAXVgaVBo1aiRjjJUlAAAAG8tTN9MCAIA7C0EFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYli2CyieffKKIiAgFBgaqbt26Wr9+vdUlAQAAG7A8qMyYMUOvvvqqBg4cqM2bNys6OlrNmzdXcnKy1aUBAACLWR5Uhg0bphdeeEFdu3ZV1apVNWbMGBUsWFDjx4+3ujQAAGAxS4PKxYsXtWnTJjVt2tTVli9fPjVt2lQ//PCDhZUBAAA78Ldy57/99psyMjJUqlQpt/ZSpUpp165dWfqnp6crPT3dtZySkiJJSk1N9Ul9mennfbLdW+GrY71VdhwrAMCt88XfnSvbNMZct6+lQeVmJSQkKD4+Pkt72bJlLajGGkWGW10BAOBO4su/O2fPnlWRIkWu2cfSoFKiRAn5+fnpxIkTbu0nTpxQaGholv79+/fXq6++6lrOzMzU77//ruLFi8vhcNzwflNTU1W2bFkdOXJEwcHBnh/AbYrxuTbG59oYn2tjfK6N8bm+22GMjDE6e/aswsLCrtvX0qASEBCg++67TytXrtSTTz4p6c/wsXLlSvXs2TNLf6fTKafT6dZWtGhRj/cfHBycZ3/IuYHxuTbG59oYn2tjfK6N8bm+vD5G15tJucLySz+vvvqqYmNjVadOHT3wwAMaPny4zp07p65du1pdGgAAsJjlQaVDhw46efKk3nnnHR0/flz33nuvlixZkuUGWwAAcOexPKhIUs+ePbO91OMrTqdTAwcOzHIZCX9ifK6N8bk2xufaGJ9rY3yu704bI4e5kWeDAAAALGD5J9MCAADkhKACAABsi6ACAABsi6ACAABs67YIKp988okiIiIUGBiounXrav369Tn2vXTpkt577z1FRkYqMDBQ0dHRWrJkiVuf0aNHq2bNmq4P06lXr54WL17s68PwGW+Pz18NGTJEDodDffr08UHlucPb4/Puu+/K4XC4vSpXruzrw/ApX5xDv/76qzp37qzixYurQIECqlGjhjZu3OjLw/AZb49PRERElnPI4XAoLi7O14fiE94en4yMDL399tsqX768ChQooMjISA0aNOiGvjfGjrw9PmfPnlWfPn0UHh6uAgUK6KGHHtKGDRt8fRi+Y/K46dOnm4CAADN+/HizY8cO88ILL5iiRYuaEydOZNv/zTffNGFhYWbRokVm//795tNPPzWBgYFm8+bNrj5fffWVWbRokdmzZ4/ZvXu3GTBggMmfP7/Zvn17bh2W1/hifK5Yv369iYiIMDVr1jS9e/f28ZH4hi/GZ+DAgaZatWomKSnJ9Tp58mRuHZLX+WKMfv/9dxMeHm66dOli/vvf/5oDBw6YpUuXmn379uXWYXmNL8YnOTnZ7fxZvny5kWRWr16dS0flPb4Yn8GDB5vixYubhQsXml9++cXMmjXLFC5c2IwYMSK3DstrfDE+zz77rKlatapZu3at2bt3rxk4cKAJDg42R48eza3D8qo8H1QeeOABExcX51rOyMgwYWFhJiEhIdv+pUuXNqNGjXJre/rpp01MTMw191OsWDHzxRdf3HrBucxX43P27FlTsWJFs3z5ctOwYcM8G1R8MT4DBw400dHRPqnXCr4Yo759+5oGDRr4puBclhu/g3r37m0iIyNNZmamd4rORb4Yn1atWplu3bpds09e4e3xOX/+vPHz8zMLFy5061O7dm3z1ltvebn63JGnL/1cvHhRmzZtUtOmTV1t+fLlU9OmTfXDDz9ku056eroCAwPd2goUKKBvv/022/4ZGRmaPn26zp07p3r16nmv+Fzgy/GJi4tTq1at3Lad1/hyfPbu3auwsDDdc889iomJ0eHDh71/ALnAV2P01VdfqU6dOnrmmWcUEhKiWrVqaezYsb45CB/Kjd9BFy9e1JQpU9StW7eb+vJVO/DV+Dz00ENauXKl9uzZI0naunWrvv32W7Vo0cIHR+E7vhify5cvKyMj46bOMbvL00Hlt99+U0ZGRpaP2y9VqpSOHz+e7TrNmzfXsGHDtHfvXmVmZmr58uWaO3eukpKS3Ppt27ZNhQsXltPp1D/+8Q/NmzdPVatW9dmx+IKvxmf69OnavHmzEhISfFq/r/lqfOrWrasJEyZoyZIlGj16tH755Rc9/PDDOnv2rE+Pxxd8NUYHDhzQ6NGjVbFiRS1dulQvvfSSevXqpYkTJ/r0eLzNl7+Drpg/f77OnDmjLl26eLt8n/PV+PTr108dO3ZU5cqVlT9/ftWqVUt9+vRRTEyMT4/H23wxPkFBQapXr54GDRqkY8eOKSMjQ1OmTNEPP/yQ4zlmd3k6qHhixIgRqlixoipXrqyAgAD17NlTXbt2Vb587kNRqVIlbdmyRf/973/10ksvKTY2Vjt37rSo6txzvfE5cuSIevfuralTp2ZJ7HeCGzl/WrRooWeeeUY1a9ZU8+bN9fXXX+vMmTOaOXOmhZXnnhsZo8zMTNWuXVsffPCBatWqpRdffFEvvPCCxowZY2HlueNGfwddMW7cOLVo0UJhYWG5XKk1bmR8Zs6cqalTp2ratGnavHmzJk6cqP/85z95Luh64kbGZ/LkyTLG6O6775bT6dTIkSPVqVOnHM8xu8ubVf//SpQoIT8/P504ccKt/cSJEwoNDc12nZIlS2r+/Pk6d+6cDh06pF27dqlw4cK655573PoFBASoQoUKuu+++5SQkKDo6GiNGDHCZ8fiC74Yn02bNik5OVm1a9eWv7+//P39tXbtWo0cOVL+/v7KyMjw+XF5iy/Pn78qWrSooqKitG/fPq/Wnxt8NUalS5fOMkNZpUqVPHeJzNfn0KFDh7RixQr16NHDJ/X7mq/G54033nDNqtSoUUPPPfecXnnllTw3y+ur8YmMjNTatWuVlpamI0eOaP369bp06dI1f0/ZWZ4OKgEBAbrvvvu0cuVKV1tmZqZWrlx53ftJAgMDdffdd+vy5cuaM2eO2rZte83+mZmZSk9P90rducUX49OkSRNt27ZNW7Zscb3q1KmjmJgYbdmyRX5+fj49Jm/KrfMnLS1N+/fvV+nSpb1We27x1RjVr19fu3fvduu/Z88ehYeHe/cAfMzX51BiYqJCQkLUqlUrr9eeG3w1PufPn88yO+Dn56fMzEzvHoCP+fr8KVSokEqXLq3Tp09r6dKl1/07Z1tW3817q6ZPn26cTqeZMGGC2blzp3nxxRdN0aJFzfHjx40xxjz33HOmX79+rv7r1q0zc+bMMfv37zfffPONefTRR0358uXN6dOnXX369etn1q5da3755Rfz008/mX79+hmHw2GWLVuW24d3y3wxPlfLy0/9+GJ8XnvtNbNmzRrzyy+/mO+++840bdrUlChRwiQnJ+f24XmFL8Zo/fr1xt/f3wwePNjs3bvXTJ061RQsWNBMmTIltw/vlvnq31hGRoYpV66c6du3b24ejtf5YnxiY2PN3Xff7Xo8ee7cuaZEiRLmzTffzO3Du2W+GJ8lS5aYxYsXmwMHDphly5aZ6OhoU7duXXPx4sXcPjyvyPNBxRhjPv74Y1OuXDkTEBBgHnjgAbNu3TrXew0bNjSxsbGu5TVr1pgqVaoYp9Npihcvbp577jnz66+/um2vW7duJjw83AQEBJiSJUuaJk2a5MmQcoW3x+dqeTmoGOP98enQoYMpXbq0CQgIMHfffbfp0KFDnvx8kL/yxTn0v//7v6Z69erG6XSaypUrm88//zw3DsUnfDE+S5cuNZLM7t27c+MQfMrb45Oammp69+5typUrZwIDA80999xj3nrrLZOenp5bh+RV3h6fGTNmmHvuuccEBASY0NBQExcXZ86cOZNbh+N1DmPy6Ef5AQCA216evkcFAADc3ggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAG7ZDz/8ID8/vyzfSbNmzRo5HA6dOXMmyzoREREaPny4a9nhcLhewcHBuv/++7VgwYIs6/3xxx8aOHCgoqKi5HQ6VaJECT3zzDPasWNHlr6pqal66623VLlyZQUGBio0NFRNmzbV3LlzxWddAnkDQQXALRs3bpxefvllffPNNzp27JjH20lMTFRSUpI2btyo+vXrq3379tq2bZvr/fT0dDVt2lTjx4/X+++/rz179ujrr7/W5cuXVbduXa1bt87V98yZM3rooYc0adIk9e/fX5s3b9Y333yjDh066M0331RKSsotHTOA3OFvdQEA8ra0tDTNmDFDGzdu1PHjxzVhwgQNGDDAo20VLVpUoaGhCg0N1aBBgzRixAitXr1aNWrUkCQNHz5cP/zwg3788UdFR0dLksLDwzVnzhzVrVtX3bt31/bt2+VwODRgwAAdPHhQe/bsUVhYmGsfUVFR6tSpkwIDA2/94AH4HDMqAG7JzJkzVblyZVWqVEmdO3fW+PHjb/myyuXLlzVu3DhJUkBAgKt92rRpatasmSukXJEvXz698sor2rlzp7Zu3arMzExNnz5dMTExbiHlisKFC8vfn/9OA/IC/qUCuCXjxo1T586dJUmPP/64UlJStHbtWjVq1Oimt9WpUyf5+fnpjz/+UGZmpiIiIvTss8+63t+zZ48aN26c7bpVqlRx9QkLC9Pp06dVuXLlmz8gALbCjAoAj+3evVvr169Xp06dJEn+/v7q0KGDazbkZn300UfasmWLFi9erKpVq+qLL77QXXfd5dbnRmZruFEWuH0wowLAY+PGjdPly5fdLq8YY+R0OjVq1CgFBwdLklJSUlS0aFG3dc+cOaMiRYq4tYWGhqpChQqqUKGCEhMT1bJlS+3cuVMhISGS/ry/5Oeff862livtUVFRKlmypIoWLapdu3Z561ABWIQZFQAeuXz5siZNmqShQ4dqy5YtrtfWrVsVFhamL7/8UhUrVlS+fPm0adMmt3UPHDiglJQURUVF5bj9Bx54QPfdd58GDx7sauvYsaNWrFihrVu3uvXNzMzURx99pKpVqyo6Olr58uVTx44dNXXq1GyfQkpLS9Ply5dvcQQA5AoDAB6YN2+eCQgIMGfOnMny3ptvvmnq1KljjDHmxRdfNBEREWbBggXmwIEDZu3atebBBx80Dz74oMnMzHStI8nMmzfPbTtff/21cTqd5ujRo8YYY/744w9Tt25dU7ZsWTNz5kxz6NAhs379evPkk0+aQoUKmR9++MG17qlTp0zlypVNmTJlzMSJE82OHTvMnj17zLhx40yFChXM6dOnvT8oALzOYQwXcwHcvDZt2igzM1OLFi3K8t769etVt25dbd26VVFRURoyZIhmzJihQ4cOKTQ0VM2aNdPgwYNVokQJ1zoOh0Pz5s3Tk08+6Wozxqhq1apq3LixPv30U0nS+fPn9cEHH7i2FxQUpMaNG+vdd99V9erV3epISUnRkCFDNGfOHB06dEjFihVTjRo1FBcXp7Zt28rhcPhmcAB4DUEFAADYFveoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2/r/AHDnn7HF7sryAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = []\n",
    "repeats = 50\n",
    "val_loader, train_loader = dataloader_factory(42)\n",
    "\n",
    "for _ in range(repeats):\n",
    "    model, optimiser, lr_scheduler = model_factory()\n",
    "    for epoch in range(epochs):\n",
    "        model = train_one_epoch(model, train_loader, optimiser, lr_scheduler, loss_fn, epoch, verbose=False)\n",
    "    _, auroc, _ = evaluate(model, val_loader, loss_fn)\n",
    "    results.append(auroc)\n",
    "\n",
    "plt.hist(results, bins=12)\n",
    "plt.xlabel('AUROC')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of AUROC results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mitigate the problem, we adapt two techniques:\n",
    "\n",
    "- Ensembling models trained on different folds of train-val data. Since the training is so fast, fitting a few addtional models is not a big deal. The train-val splitting method is provided by TDC.\n",
    "\n",
    "- Rather than choosing the model at the last epoch, we will use best validation loss to decide which one to choose.\n",
    "\n",
    "Below we implement a method that create a new training and validation dataloader for each fold, and also a method for ensemble-based evaluationm, where probabiltiies for each predition are averaged across all models in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(predictors, dataloader, loss_fn):\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            model_outputs = [predictor(inputs).squeeze() for predictor in predictors]\n",
    "            averaged_output = torch.sigmoid(torch.mean(torch.stack(model_outputs), dim=0))\n",
    "\n",
    "            loss = loss_fn(averaged_output, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_probs.extend(averaged_output.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    loss = total_loss / len(all_probs)\n",
    "    return loss, roc_auc_score(all_targets, all_probs), average_precision_score(all_targets, all_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how much better our model gets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3212.96it/s]\n",
      "featurizing_smiles, batch=1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 906.63it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 10697.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.39it/s]\n",
      "featurizing_smiles, batch=13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 126.53it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 9506.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.0137\tval_loss: 0.0083\tval_auroc: 0.6162\tval_avpr: 0.9296\n",
      "## Epoch 2\ttrain_loss: 0.0078\tval_loss: 0.0056\tval_auroc: 0.8796\tval_avpr: 0.9819\n",
      "## Epoch 3\ttrain_loss: 0.0052\tval_loss: 0.0040\tval_auroc: 0.9300\tval_avpr: 0.9900\n",
      "## Epoch 4\ttrain_loss: 0.0037\tval_loss: 0.0034\tval_auroc: 0.9552\tval_avpr: 0.9938\n",
      "## Epoch 5\ttrain_loss: 0.0028\tval_loss: 0.0031\tval_auroc: 0.9636\tval_avpr: 0.9951\n",
      "## Epoch 6\ttrain_loss: 0.0022\tval_loss: 0.0028\tval_auroc: 0.9692\tval_avpr: 0.9959\n",
      "## Epoch 7\ttrain_loss: 0.0014\tval_loss: 0.0028\tval_auroc: 0.9720\tval_avpr: 0.9963\n",
      "## Epoch 8\ttrain_loss: 0.0015\tval_loss: 0.0028\tval_auroc: 0.9748\tval_avpr: 0.9967\n",
      "## Epoch 9\ttrain_loss: 0.0012\tval_loss: 0.0029\tval_auroc: 0.9664\tval_avpr: 0.9955\n",
      "## Epoch 10\ttrain_loss: 0.0009\tval_loss: 0.0028\tval_auroc: 0.9692\tval_avpr: 0.9958\n",
      "## Epoch 11\ttrain_loss: 0.0009\tval_loss: 0.0026\tval_auroc: 0.9748\tval_avpr: 0.9966\n",
      "## Epoch 12\ttrain_loss: 0.0007\tval_loss: 0.0026\tval_auroc: 0.9748\tval_avpr: 0.9966\n",
      "## Epoch 13\ttrain_loss: 0.0007\tval_loss: 0.0025\tval_auroc: 0.9748\tval_avpr: 0.9966\n",
      "## Epoch 14\ttrain_loss: 0.0006\tval_loss: 0.0025\tval_auroc: 0.9748\tval_avpr: 0.9966\n",
      "## Epoch 15\ttrain_loss: 0.0006\tval_loss: 0.0026\tval_auroc: 0.9692\tval_avpr: 0.9958\n",
      "## Epoch 16\ttrain_loss: 0.0006\tval_loss: 0.0026\tval_auroc: 0.9776\tval_avpr: 0.9970\n",
      "## Epoch 17\ttrain_loss: 0.0005\tval_loss: 0.0026\tval_auroc: 0.9776\tval_avpr: 0.9970\n",
      "## Epoch 18\ttrain_loss: 0.0004\tval_loss: 0.0025\tval_auroc: 0.9748\tval_avpr: 0.9967\n",
      "## Epoch 19\ttrain_loss: 0.0004\tval_loss: 0.0026\tval_auroc: 0.9804\tval_avpr: 0.9974\n",
      "## Epoch 20\ttrain_loss: 0.0005\tval_loss: 0.0026\tval_auroc: 0.9776\tval_avpr: 0.9970\n",
      "## Epoch 21\ttrain_loss: 0.0005\tval_loss: 0.0026\tval_auroc: 0.9804\tval_avpr: 0.9974\n",
      "## Epoch 22\ttrain_loss: 0.0004\tval_loss: 0.0026\tval_auroc: 0.9776\tval_avpr: 0.9970\n",
      "## Epoch 23\ttrain_loss: 0.0005\tval_loss: 0.0026\tval_auroc: 0.9720\tval_avpr: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 24\ttrain_loss: 0.0006\tval_loss: 0.0025\tval_auroc: 0.9748\tval_avpr: 0.9966\n",
      "## Epoch 25\ttrain_loss: 0.0004\tval_loss: 0.0025\tval_auroc: 0.9748\tval_avpr: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3609.71it/s]\n",
      "featurizing_smiles, batch=1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 772.64it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 11284.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.47it/s]\n",
      "featurizing_smiles, batch=13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 64.24it/s] \n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 14826.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.0165\tval_loss: 0.0085\tval_auroc: 0.8528\tval_avpr: 0.9818\n",
      "## Epoch 2\ttrain_loss: 0.0089\tval_loss: 0.0055\tval_auroc: 0.9736\tval_avpr: 0.9975\n",
      "## Epoch 3\ttrain_loss: 0.0057\tval_loss: 0.0037\tval_auroc: 0.9698\tval_avpr: 0.9971\n",
      "## Epoch 4\ttrain_loss: 0.0041\tval_loss: 0.0029\tval_auroc: 0.9736\tval_avpr: 0.9975\n",
      "## Epoch 5\ttrain_loss: 0.0031\tval_loss: 0.0024\tval_auroc: 0.9849\tval_avpr: 0.9986\n",
      "## Epoch 6\ttrain_loss: 0.0023\tval_loss: 0.0022\tval_auroc: 0.9849\tval_avpr: 0.9986\n",
      "## Epoch 7\ttrain_loss: 0.0019\tval_loss: 0.0020\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 8\ttrain_loss: 0.0015\tval_loss: 0.0020\tval_auroc: 0.9887\tval_avpr: 0.9990\n",
      "## Epoch 9\ttrain_loss: 0.0016\tval_loss: 0.0018\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 10\ttrain_loss: 0.0010\tval_loss: 0.0019\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 11\ttrain_loss: 0.0011\tval_loss: 0.0018\tval_auroc: 0.9887\tval_avpr: 0.9989\n",
      "## Epoch 12\ttrain_loss: 0.0009\tval_loss: 0.0019\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 13\ttrain_loss: 0.0007\tval_loss: 0.0018\tval_auroc: 0.9887\tval_avpr: 0.9989\n",
      "## Epoch 14\ttrain_loss: 0.0008\tval_loss: 0.0018\tval_auroc: 0.9887\tval_avpr: 0.9989\n",
      "## Epoch 15\ttrain_loss: 0.0007\tval_loss: 0.0019\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 16\ttrain_loss: 0.0006\tval_loss: 0.0016\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 17\ttrain_loss: 0.0006\tval_loss: 0.0018\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 18\ttrain_loss: 0.0007\tval_loss: 0.0016\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 19\ttrain_loss: 0.0005\tval_loss: 0.0017\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 20\ttrain_loss: 0.0006\tval_loss: 0.0017\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 21\ttrain_loss: 0.0006\tval_loss: 0.0018\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 22\ttrain_loss: 0.0006\tval_loss: 0.0017\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 23\ttrain_loss: 0.0006\tval_loss: 0.0017\tval_auroc: 0.9925\tval_avpr: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 24\ttrain_loss: 0.0006\tval_loss: 0.0017\tval_auroc: 0.9925\tval_avpr: 0.9993\n",
      "## Epoch 25\ttrain_loss: 0.0005\tval_loss: 0.0016\tval_auroc: 0.9925\tval_avpr: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3572.99it/s]\n",
      "featurizing_smiles, batch=1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 863.37it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 10101.30it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.59it/s]\n",
      "featurizing_smiles, batch=13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 120.44it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 14641.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.0171\tval_loss: 0.0085\tval_auroc: 0.7051\tval_avpr: 0.9304\n",
      "## Epoch 2\ttrain_loss: 0.0088\tval_loss: 0.0058\tval_auroc: 0.8590\tval_avpr: 0.9800\n",
      "## Epoch 3\ttrain_loss: 0.0059\tval_loss: 0.0042\tval_auroc: 0.9103\tval_avpr: 0.9887\n",
      "## Epoch 4\ttrain_loss: 0.0041\tval_loss: 0.0035\tval_auroc: 0.9103\tval_avpr: 0.9886\n",
      "## Epoch 5\ttrain_loss: 0.0029\tval_loss: 0.0034\tval_auroc: 0.8910\tval_avpr: 0.9832\n",
      "## Epoch 6\ttrain_loss: 0.0025\tval_loss: 0.0032\tval_auroc: 0.9231\tval_avpr: 0.9898\n",
      "## Epoch 7\ttrain_loss: 0.0018\tval_loss: 0.0032\tval_auroc: 0.8814\tval_avpr: 0.9802\n",
      "## Epoch 8\ttrain_loss: 0.0015\tval_loss: 0.0031\tval_auroc: 0.8718\tval_avpr: 0.9755\n",
      "## Epoch 9\ttrain_loss: 0.0013\tval_loss: 0.0031\tval_auroc: 0.8878\tval_avpr: 0.9809\n",
      "## Epoch 10\ttrain_loss: 0.0011\tval_loss: 0.0027\tval_auroc: 0.8910\tval_avpr: 0.9818\n",
      "## Epoch 11\ttrain_loss: 0.0009\tval_loss: 0.0028\tval_auroc: 0.8910\tval_avpr: 0.9818\n",
      "## Epoch 12\ttrain_loss: 0.0009\tval_loss: 0.0029\tval_auroc: 0.8942\tval_avpr: 0.9827\n",
      "## Epoch 13\ttrain_loss: 0.0007\tval_loss: 0.0029\tval_auroc: 0.9038\tval_avpr: 0.9851\n",
      "## Epoch 14\ttrain_loss: 0.0007\tval_loss: 0.0029\tval_auroc: 0.9071\tval_avpr: 0.9858\n",
      "## Epoch 15\ttrain_loss: 0.0007\tval_loss: 0.0030\tval_auroc: 0.8942\tval_avpr: 0.9827\n",
      "## Epoch 16\ttrain_loss: 0.0006\tval_loss: 0.0030\tval_auroc: 0.8654\tval_avpr: 0.9728\n",
      "## Epoch 17\ttrain_loss: 0.0009\tval_loss: 0.0030\tval_auroc: 0.8558\tval_avpr: 0.9694\n",
      "## Epoch 18\ttrain_loss: 0.0009\tval_loss: 0.0031\tval_auroc: 0.8558\tval_avpr: 0.9694\n",
      "## Epoch 19\ttrain_loss: 0.0008\tval_loss: 0.0031\tval_auroc: 0.8782\tval_avpr: 0.9785\n",
      "## Epoch 20\ttrain_loss: 0.0005\tval_loss: 0.0030\tval_auroc: 0.8814\tval_avpr: 0.9796\n",
      "## Epoch 21\ttrain_loss: 0.0005\tval_loss: 0.0030\tval_auroc: 0.8878\tval_avpr: 0.9814\n",
      "## Epoch 22\ttrain_loss: 0.0005\tval_loss: 0.0030\tval_auroc: 0.8878\tval_avpr: 0.9814\n",
      "## Epoch 23\ttrain_loss: 0.0007\tval_loss: 0.0030\tval_auroc: 0.8814\tval_avpr: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 24\ttrain_loss: 0.0005\tval_loss: 0.0030\tval_auroc: 0.8814\tval_avpr: 0.9796\n",
      "## Epoch 25\ttrain_loss: 0.0005\tval_loss: 0.0030\tval_auroc: 0.8814\tval_avpr: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3578.86it/s]\n",
      "featurizing_smiles, batch=1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 914.58it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:00<00:00, 13169.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.78it/s]\n",
      "featurizing_smiles, batch=13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 125.15it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 14221.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.0215\tval_loss: 0.0112\tval_auroc: 0.5370\tval_avpr: 0.9444\n",
      "## Epoch 2\ttrain_loss: 0.0115\tval_loss: 0.0070\tval_auroc: 0.7870\tval_avpr: 0.9688\n",
      "## Epoch 3\ttrain_loss: 0.0069\tval_loss: 0.0045\tval_auroc: 0.8380\tval_avpr: 0.9821\n",
      "## Epoch 4\ttrain_loss: 0.0048\tval_loss: 0.0033\tval_auroc: 0.8472\tval_avpr: 0.9838\n",
      "## Epoch 5\ttrain_loss: 0.0038\tval_loss: 0.0031\tval_auroc: 0.8241\tval_avpr: 0.9793\n",
      "## Epoch 6\ttrain_loss: 0.0029\tval_loss: 0.0028\tval_auroc: 0.8287\tval_avpr: 0.9803\n",
      "## Epoch 7\ttrain_loss: 0.0022\tval_loss: 0.0027\tval_auroc: 0.8241\tval_avpr: 0.9800\n",
      "## Epoch 8\ttrain_loss: 0.0016\tval_loss: 0.0028\tval_auroc: 0.8380\tval_avpr: 0.9821\n",
      "## Epoch 9\ttrain_loss: 0.0016\tval_loss: 0.0027\tval_auroc: 0.8241\tval_avpr: 0.9793\n",
      "## Epoch 10\ttrain_loss: 0.0015\tval_loss: 0.0026\tval_auroc: 0.8380\tval_avpr: 0.9826\n",
      "## Epoch 11\ttrain_loss: 0.0011\tval_loss: 0.0026\tval_auroc: 0.8472\tval_avpr: 0.9842\n",
      "## Epoch 12\ttrain_loss: 0.0011\tval_loss: 0.0027\tval_auroc: 0.8519\tval_avpr: 0.9849\n",
      "## Epoch 13\ttrain_loss: 0.0010\tval_loss: 0.0026\tval_auroc: 0.8380\tval_avpr: 0.9826\n",
      "## Epoch 14\ttrain_loss: 0.0009\tval_loss: 0.0025\tval_auroc: 0.8380\tval_avpr: 0.9826\n",
      "## Epoch 15\ttrain_loss: 0.0008\tval_loss: 0.0025\tval_auroc: 0.8333\tval_avpr: 0.9818\n",
      "## Epoch 16\ttrain_loss: 0.0007\tval_loss: 0.0025\tval_auroc: 0.8472\tval_avpr: 0.9842\n",
      "## Epoch 17\ttrain_loss: 0.0007\tval_loss: 0.0025\tval_auroc: 0.8287\tval_avpr: 0.9809\n",
      "## Epoch 18\ttrain_loss: 0.0007\tval_loss: 0.0025\tval_auroc: 0.8472\tval_avpr: 0.9842\n",
      "## Epoch 19\ttrain_loss: 0.0006\tval_loss: 0.0025\tval_auroc: 0.8333\tval_avpr: 0.9818\n",
      "## Epoch 20\ttrain_loss: 0.0007\tval_loss: 0.0026\tval_auroc: 0.8333\tval_avpr: 0.9818\n",
      "## Epoch 21\ttrain_loss: 0.0009\tval_loss: 0.0025\tval_auroc: 0.8380\tval_avpr: 0.9826\n",
      "## Epoch 22\ttrain_loss: 0.0006\tval_loss: 0.0025\tval_auroc: 0.8333\tval_avpr: 0.9818\n",
      "## Epoch 23\ttrain_loss: 0.0007\tval_loss: 0.0025\tval_auroc: 0.8333\tval_avpr: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 24\ttrain_loss: 0.0006\tval_loss: 0.0025\tval_auroc: 0.8333\tval_avpr: 0.9818\n",
      "## Epoch 25\ttrain_loss: 0.0006\tval_loss: 0.0025\tval_auroc: 0.8333\tval_avpr: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 3562.63it/s]\n",
      "featurizing_smiles, batch=2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 601.66it/s]\n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 13423.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.94it/s]\n",
      "featurizing_smiles, batch=13: 31it [00:00, 123.61it/s]                        \n",
      "Casting to FP32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 397/397 [00:00<00:00, 20337.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.0150\tval_loss: 0.0079\tval_auroc: 0.5238\tval_avpr: 0.8859\n",
      "## Epoch 2\ttrain_loss: 0.0080\tval_loss: 0.0054\tval_auroc: 0.7343\tval_avpr: 0.9434\n",
      "## Epoch 3\ttrain_loss: 0.0054\tval_loss: 0.0040\tval_auroc: 0.8045\tval_avpr: 0.9655\n",
      "## Epoch 4\ttrain_loss: 0.0039\tval_loss: 0.0036\tval_auroc: 0.8195\tval_avpr: 0.9686\n",
      "## Epoch 5\ttrain_loss: 0.0027\tval_loss: 0.0035\tval_auroc: 0.8271\tval_avpr: 0.9716\n",
      "## Epoch 6\ttrain_loss: 0.0019\tval_loss: 0.0035\tval_auroc: 0.8170\tval_avpr: 0.9691\n",
      "## Epoch 7\ttrain_loss: 0.0018\tval_loss: 0.0033\tval_auroc: 0.8396\tval_avpr: 0.9745\n",
      "## Epoch 8\ttrain_loss: 0.0014\tval_loss: 0.0034\tval_auroc: 0.8471\tval_avpr: 0.9769\n",
      "## Epoch 9\ttrain_loss: 0.0010\tval_loss: 0.0035\tval_auroc: 0.8521\tval_avpr: 0.9775\n",
      "## Epoch 10\ttrain_loss: 0.0009\tval_loss: 0.0036\tval_auroc: 0.8396\tval_avpr: 0.9750\n",
      "## Epoch 11\ttrain_loss: 0.0008\tval_loss: 0.0036\tval_auroc: 0.8471\tval_avpr: 0.9763\n",
      "## Epoch 12\ttrain_loss: 0.0006\tval_loss: 0.0036\tval_auroc: 0.8521\tval_avpr: 0.9774\n",
      "## Epoch 13\ttrain_loss: 0.0006\tval_loss: 0.0036\tval_auroc: 0.8471\tval_avpr: 0.9762\n",
      "## Epoch 14\ttrain_loss: 0.0008\tval_loss: 0.0036\tval_auroc: 0.8371\tval_avpr: 0.9740\n",
      "## Epoch 15\ttrain_loss: 0.0006\tval_loss: 0.0035\tval_auroc: 0.8496\tval_avpr: 0.9765\n",
      "## Epoch 16\ttrain_loss: 0.0006\tval_loss: 0.0035\tval_auroc: 0.8496\tval_avpr: 0.9770\n",
      "## Epoch 17\ttrain_loss: 0.0008\tval_loss: 0.0036\tval_auroc: 0.8571\tval_avpr: 0.9781\n",
      "## Epoch 18\ttrain_loss: 0.0006\tval_loss: 0.0037\tval_auroc: 0.8647\tval_avpr: 0.9795\n",
      "## Epoch 19\ttrain_loss: 0.0005\tval_loss: 0.0037\tval_auroc: 0.8521\tval_avpr: 0.9773\n",
      "## Epoch 20\ttrain_loss: 0.0005\tval_loss: 0.0036\tval_auroc: 0.8521\tval_avpr: 0.9773\n",
      "## Epoch 21\ttrain_loss: 0.0004\tval_loss: 0.0037\tval_auroc: 0.8421\tval_avpr: 0.9751\n",
      "## Epoch 22\ttrain_loss: 0.0005\tval_loss: 0.0037\tval_auroc: 0.8421\tval_avpr: 0.9751\n",
      "## Epoch 23\ttrain_loss: 0.0007\tval_loss: 0.0037\tval_auroc: 0.8421\tval_avpr: 0.9750\n",
      "## Epoch 24\ttrain_loss: 0.0005\tval_loss: 0.0037\tval_auroc: 0.8396\tval_avpr: 0.9744\n",
      "## Epoch 25\ttrain_loss: 0.0006\tval_loss: 0.0036\tval_auroc: 0.8396\tval_avpr: 0.9747\n",
      "test_loss: 0.0015\n",
      "test_auroc: 0.9844\n",
      "test_avpr: 0.9953\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "best_models = []\n",
    "\n",
    "for seed in seeds:\n",
    "    val_loader, train_loader = dataloader_factory(seed)\n",
    "    model, optimiser, lr_scheduler = model_factory()\n",
    "\n",
    "    best_epoch = {\"model\": None, \"result\": None}\n",
    "    for epoch in range(epochs):\n",
    "        model = train_one_epoch(model, train_loader, optimiser, lr_scheduler, loss_fn, epoch)\n",
    "        val_loss, val_auroc, _ = evaluate(model, val_loader, loss_fn)\n",
    "\n",
    "        if best_epoch['model'] is None:\n",
    "            best_epoch['model'] = deepcopy(model)\n",
    "            best_epoch['result'] = val_loss\n",
    "        else:\n",
    "            best_epoch['model'] = best_epoch['model'] if best_epoch['result'] <= val_loss else deepcopy(model)\n",
    "            best_epoch['result'] = best_epoch['result'] if best_epoch['result'] <= val_loss else val_loss \n",
    "\n",
    "    best_models.append(best_epoch['model'])\n",
    "\n",
    "test_loss, test_auroc, test_avpr = evaluate_ensemble(best_models, test_loader, loss_fn)\n",
    "print(\n",
    "    f\"test_loss: {test_loss:.4f}\\n\"\n",
    "    f\"test_auroc: {test_auroc:.4f}\\n\"\n",
    "    f\"test_avpr: {test_avpr:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in less than 15s the ensemble was trained and reached performance of 0.9844 in AUROC on the test set. That's a decent improvement coming from 0.9683 that we got for a single model. More importantly, the ensemble is not senstitive to which part of the data is used for validation (because we train n models on n folds), and is less sensitive to the intialisation because we intialise n models getting somewhere close to an average performance. \n",
    "\n",
    "The results could be further improved by sweeping hyperparameters. Considering the low cost of fitting these tiny models, it would still be very fast. The results reported in the paper are slighly better than what we show here, thanks to sweeping that we performed for each task.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
