{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blazejb/minimol/.minimol/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, input_dim=512, head_hidden_dim=256, dropout=0.1, task_names=None):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        self.dense1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            task_name: nn.Sequential(\n",
    "                nn.Linear(hidden_dim, head_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(head_hidden_dim, 1)\n",
    "            ) for task_name in task_names\n",
    "        })\n",
    "\n",
    "        self.trunk_frozen = False\n",
    "\n",
    "    def forward(self, x, task_name):\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.heads[task_name](x)\n",
    "        return x\n",
    "\n",
    "    def freeze_trunk(self):\n",
    "        self.trunk_frozen = True\n",
    "        for param in self.dense1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.dense2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bn1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bn2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_trunk(self):\n",
    "        self.trunk_frozen = False\n",
    "        for param in self.dense1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.dense2.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.bn1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.bn2.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "def model_factory(lr=3e-3, epochs=25, warmup=5, weight_decay=1e-4):\n",
    "    model = MultiTaskModel()\n",
    "    optimiser = optim.adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def lr_fn(epoch):\n",
    "        if epoch < warmup: return epoch / warmup\n",
    "        else: return (1 + math.cos(math.pi * (epoch - warmup) / (epochs - warmup))) / 2\n",
    "\n",
    "    lr_scheduler = LambdaLR(optimiser, lr_lambda=lr_fn)\n",
    "    return model, optimiser, lr_scheduler\n",
    "\n",
    "\n",
    "def evaluate(predictor, task, eval_type='val'):\n",
    "    predictor.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    dataloader = task.val_dataloader if eval_type == 'val' else task.test_dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            logits = predictor(inputs, task_name=task.name).squeeze()\n",
    "            loss = task.get_loss(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def evaluate_ensemble(predictors, dataloader, task):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, _ in dataloader:\n",
    "            ensemble_logits = [predictor(inputs).squeeze() for predictor in predictors]\n",
    "            averaged_logits = torch.mean(torch.stack(ensemble_logits), dim=0)\n",
    "            if task == 'classification':\n",
    "                predictions += torch.sigmoid(averaged_logits)\n",
    "            else:\n",
    "                predictions += averaged_logits\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def train_one_epoch(predictor, task, optimiser):\n",
    "    train_loss = 0\n",
    "        \n",
    "    for inputs, targets in task.train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        logits = predictor(inputs, task_name=task.name).squeeze()\n",
    "        loss = task.get_loss(logits, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return predictor, train_loss / len(task.train_loader)\n",
    "\n",
    "\n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['Embedding'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, dataset_name, featuriser):\n",
    "        benchmark = group.get(dataset_name)\n",
    "        with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull): # suppress output\n",
    "            mols_test               = benchmark['test']\n",
    "            mols_train, mols_valid  = group.get_train_valid_split(benchmark=dataset_name, seed=42)\n",
    "            mols_test['Embedding']  = featuriser(list(mols_test['Drug']))\n",
    "            mols_train['Embedding'] = featuriser(list(mols_train['Drug']))\n",
    "            mols_valid['Embedding'] = featuriser(list(mols_valid['Drug']))\n",
    "        self.name         = dataset_name\n",
    "        self.test_loader  = DataLoader(AdmetDataset(mols_test), batch_size=128, shuffle=False)\n",
    "        self.val_loader   = DataLoader(AdmetDataset(mols_valid), batch_size=128, shuffle=False)\n",
    "        self.train_loader = DataLoader(AdmetDataset(mols_train), batch_size=32, shuffle=True)\n",
    "        self.task         = 'classification' if len(benchmark['test']['Y'].unique()) == 2 else 'regression'\n",
    "        self.loss_fn      = nn.BCELoss() if self.task == 'classification' else nn.MSELoss()        \n",
    "\n",
    "    def get_loss(self, logits, targets):\n",
    "        if self.task == 'classification':\n",
    "            return self.loss_fn(torch.sigmoid(logits), targets)\n",
    "        else:\n",
    "            return self.loss_fn(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurising datasets\n",
      "dataset=1 / 22\n",
      "dataset=2 / 22\n",
      "dataset=3 / 22\n",
      "dataset=4 / 22\n",
      "dataset=5 / 22\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "group = admet_group(path='admet_data/')\n",
    "featuriser = Minimol()\n",
    "tasks = {}\n",
    "\n",
    "print('featurising datasets')\n",
    "for dataset_i, dataset_name in enumerate(group.dataset_names):\n",
    "    print(f'dataset={dataset_i + 1} / {len(group.dataset_names)}')\n",
    "    tasks[dataset_name] = Task(dataset_name, featuriser) \n",
    "\n",
    "del featuriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimiser, lr_scheduler = model_factory()\n",
    "\n",
    "model.unfreeze_trunk()\n",
    "for epoch in range(EPOCHS):\n",
    "    for task_i, (task_name, task) in enumerate(tasks.items()):\n",
    "        #lr_scheduler.step(epoch)\n",
    "        model, train_loss = train_one_epoch(model, task, optimiser, lr_scheduler)\n",
    "        val_loss = evaluate(model, task, eval_type='val')\n",
    "        print(f'epoch={epoch+1} / {EPOCHS} | {task_name=} | {train_loss:.4f=} | {val_loss:.4f=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
